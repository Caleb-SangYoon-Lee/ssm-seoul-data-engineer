{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fc7bd9d-11ba-439f-a0cc-9931ddd15074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://60be28344df5:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f0623db7460>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from IPython.display import display, display_pretty, clear_output, JSON\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.sql.session.timeZone\", \"Asia/Seoul\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# 노트북에서 테이블 형태로 데이터 프레임 출력을 위한 설정을 합니다\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # display enabled\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.truncate\", 100) # display output columns size\n",
    "\n",
    "# 공통 데이터 위치\n",
    "home_jovyan = \"/home/jovyan\"\n",
    "work_data = f\"{home_jovyan}/work/data\"\n",
    "work_dir=!pwd\n",
    "work_dir = work_dir[0]\n",
    "\n",
    "# 로컬 환경 최적화\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5) # the number of partitions to use when shuffling data for joins or aggregations.\n",
    "spark.conf.set(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\")\n",
    "\n",
    "# 현재 기동된 스파크 애플리케이션의 포트를 확인하기 위해 스파크 정보를 출력합니다\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe58f60d-56d7-46ff-838a-f6ffe82c74ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스트림 테이블을 주기적으로 조회하는 함수 (name: 이름, sql: Spark SQL, iterations: 반복횟수, sleep_secs: 인터벌)\n",
    "def displayStream(name, sql, iterations, sleep_secs):\n",
    "    from time import sleep\n",
    "    i = 1\n",
    "    for x in range(iterations):\n",
    "        clear_output(wait=True)              # 출력 Cell 을 지웁니다\n",
    "        display('[' + name + '] Iteration: '+str(i)+', Query: '+sql)\n",
    "        display(spark.sql(sql))              # Spark SQL 을 수행합니다\n",
    "        sleep(sleep_secs)                    # sleep_secs 초 만큼 대기합니다\n",
    "        i += 1\n",
    "\n",
    "# 스트림 쿼리의 상태를 주기적으로 조회하는 함수 (name: 이름, query: Streaming Query, iterations: 반복횟수, sleep_secs: 인터벌)\n",
    "def displayStatus(name, query, iterations, sleep_secs):\n",
    "    from time import sleep\n",
    "    i = 1\n",
    "    for x in range(iterations):\n",
    "        clear_output(wait=True)      # Output Cell 의 내용을 지웁니다\n",
    "        display('[' + name + '] Iteration: '+str(i)+', Status: '+query.status['message'])\n",
    "        display(query.lastProgress)  # 마지막 수행된 쿼리의 상태를 출력합니다\n",
    "        sleep(sleep_secs)            # 지정된 시간(초)을 대기합니다\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b611dba8-b1f0-4f87-adc9-f27355212671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- mod_id: long (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- hello: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# {\"hello\":\"ssm-seoul\",\"id\":0,\"time\":\"2022-09-30 16:42:01\"}\n",
    "\n",
    "kafkaReader = (\n",
    "    spark\n",
    "  .readStream\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka:9093\")\n",
    "  .option(\"subscribe\", \"events\")\n",
    "  .option(\"startingOffsets\", \"latest\")\n",
    "  .load()\n",
    ")\n",
    "kafkaReader.printSchema()\n",
    "\n",
    "kafkaSchema = (\n",
    "    StructType()\n",
    "    .add(StructField(\"id\", LongType()))\n",
    "    .add(StructField(\"hello\", StringType()))\n",
    "    .add(StructField(\"time\", StringType()))\n",
    ")\n",
    "\n",
    "kafkaSelector = (\n",
    "    kafkaReader\n",
    "    .select(\n",
    "        col(\"key\").cast(\"string\"),\n",
    "        from_json(col(\"value\").cast(\"string\"), kafkaSchema).alias(\"events\")\n",
    "    )\n",
    "    .selectExpr(\"events.id % 10 as mod_id\", \"events.*\")\n",
    ")\n",
    "\n",
    "kafkaSelector.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5b02c63d-c5e0-42b0-9262-d9e346ce2c08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[consoleSink] Iteration: 4, Query: select * from consoleSink order by mod_id desc'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>mod_id</th><th>id</th><th>hello</th><th>time</th></tr>\n",
       "<tr><td>9</td><td>2849</td><td>ssm-seoul</td><td>2022-09-30 17:38:46</td></tr>\n",
       "<tr><td>9</td><td>2859</td><td>ssm-seoul</td><td>2022-09-30 17:38:56</td></tr>\n",
       "<tr><td>8</td><td>2848</td><td>ssm-seoul</td><td>2022-09-30 17:38:45</td></tr>\n",
       "<tr><td>8</td><td>2858</td><td>ssm-seoul</td><td>2022-09-30 17:38:55</td></tr>\n",
       "<tr><td>7</td><td>2857</td><td>ssm-seoul</td><td>2022-09-30 17:38:54</td></tr>\n",
       "<tr><td>6</td><td>2856</td><td>ssm-seoul</td><td>2022-09-30 17:38:53</td></tr>\n",
       "<tr><td>5</td><td>2855</td><td>ssm-seoul</td><td>2022-09-30 17:38:52</td></tr>\n",
       "<tr><td>4</td><td>2854</td><td>ssm-seoul</td><td>2022-09-30 17:38:51</td></tr>\n",
       "<tr><td>3</td><td>2853</td><td>ssm-seoul</td><td>2022-09-30 17:38:50</td></tr>\n",
       "<tr><td>2</td><td>2852</td><td>ssm-seoul</td><td>2022-09-30 17:38:49</td></tr>\n",
       "<tr><td>1</td><td>2851</td><td>ssm-seoul</td><td>2022-09-30 17:38:48</td></tr>\n",
       "<tr><td>0</td><td>2850</td><td>ssm-seoul</td><td>2022-09-30 17:38:47</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+------+----+---------+-------------------+\n",
       "|mod_id|  id|    hello|               time|\n",
       "+------+----+---------+-------------------+\n",
       "|     9|2849|ssm-seoul|2022-09-30 17:38:46|\n",
       "|     9|2859|ssm-seoul|2022-09-30 17:38:56|\n",
       "|     8|2848|ssm-seoul|2022-09-30 17:38:45|\n",
       "|     8|2858|ssm-seoul|2022-09-30 17:38:55|\n",
       "|     7|2857|ssm-seoul|2022-09-30 17:38:54|\n",
       "|     6|2856|ssm-seoul|2022-09-30 17:38:53|\n",
       "|     5|2855|ssm-seoul|2022-09-30 17:38:52|\n",
       "|     4|2854|ssm-seoul|2022-09-30 17:38:51|\n",
       "|     3|2853|ssm-seoul|2022-09-30 17:38:50|\n",
       "|     2|2852|ssm-seoul|2022-09-30 17:38:49|\n",
       "|     1|2851|ssm-seoul|2022-09-30 17:38:48|\n",
       "|     0|2850|ssm-seoul|2022-09-30 17:38:47|\n",
       "+------+----+---------+-------------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 노트북 로그 콘솔로 출력\n",
    "queryName = \"consoleSink\"\n",
    "kafkaWriter = (\n",
    "    kafkaSelector.select(\"*\")\n",
    "    .writeStream\n",
    "    .queryName(queryName)\n",
    "    .format(\"memory\")\n",
    "    .outputMode(\"append\")\n",
    ")\n",
    "\n",
    "checkpointLocation = f\"{work_dir}/tmp/{queryName}\"\n",
    "!rm -rf $checkpointLocation\n",
    "\n",
    "kafkaTrigger = (\n",
    "    kafkaWriter\n",
    "    .trigger(processingTime=\"5 second\")\n",
    "    .option(\"checkpointLocation\", checkpointLocation)\n",
    ")\n",
    "\n",
    "# 파이썬의 경우 콘솔 디버깅이 노트북 표준출력으로 나오기 때문에, 별도 메모리 테이블로 조회\n",
    "kafkaQuery = kafkaTrigger.start()\n",
    "displayStream(queryName, f\"select * from {queryName} order by mod_id desc\", 4, 5)\n",
    "kafkaQuery.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0298d46b-62fa-4025-93e1-6a7a5f0a9cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- uid: long (nullable = true)\n",
      "\n",
      "+-----+---+\n",
      "|name |uid|\n",
      "+-----+---+\n",
      "|zero |0  |\n",
      "|one  |1  |\n",
      "|two  |2  |\n",
      "|three|3  |\n",
      "|four |4  |\n",
      "|five |5  |\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "namePath = f\"{work_dir}/data/names\"\n",
    "nameStatic = (\n",
    "    spark\n",
    "    .read\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .json(namePath)\n",
    ")\n",
    "nameStatic.printSchema()\n",
    "nameStatic.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8186d35a-a74d-4bf1-ae13-d27521d7e4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "joinExpression = (kafkaSelector.mod_id == nameStatic.uid)\n",
    "staticSelector = kafkaSelector.join(nameStatic, joinExpression, \"leftOuter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "89c721ec-c79a-4514-8c24-9f12623abe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "queryName = \"memorySink\"\n",
    "staticWriter = (\n",
    "    staticSelector\n",
    "    .selectExpr(\"time\", \"id as user_id\", \"name as user_name\", \"hello\", \"uid\")\n",
    "    .selectExpr(\"user_id as key\", \"to_json(struct(*)) as value\")\n",
    "    .writeStream\n",
    "    .queryName(queryName)\n",
    "    .format(\"memory\")\n",
    "    .outputMode(\"append\")\n",
    ")\n",
    "\n",
    "checkpointLocation = f\"{work_dir}/tmp/{queryName}\"\n",
    "!rm -rf $checkpointLocation\n",
    "\n",
    "staticTrigger = (\n",
    "    staticWriter\n",
    "    .trigger(processingTime=\"5 second\")\n",
    "    .option(\"checkpointLocation\", checkpointLocation)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "81a7e1f0-ec59-4709-bbf6-9282f3a96f8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[kafkaSink] Iteration: 4, Query: select * from kafkaSink order by key desc'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>key</th><th>value</th></tr>\n",
       "<tr><td>2959</td><td>{&quot;time&quot;:&quot;2022-09-30 17:40:36&quot;,&quot;user_id&quot;:2959,&quot;hello&quot;:&quot;ssm-seoul&quot;}</td></tr>\n",
       "<tr><td>2958</td><td>{&quot;time&quot;:&quot;2022-09-30 17:40:35&quot;,&quot;user_id&quot;:2958,&quot;hello&quot;:&quot;ssm-seoul&quot;}</td></tr>\n",
       "<tr><td>2957</td><td>{&quot;time&quot;:&quot;2022-09-30 17:40:34&quot;,&quot;user_id&quot;:2957,&quot;hello&quot;:&quot;ssm-seoul&quot;}</td></tr>\n",
       "<tr><td>2956</td><td>{&quot;time&quot;:&quot;2022-09-30 17:40:33&quot;,&quot;user_id&quot;:2956,&quot;hello&quot;:&quot;ssm-seoul&quot;}</td></tr>\n",
       "<tr><td>2955</td><td>{&quot;time&quot;:&quot;2022-09-30 17:40:32&quot;,&quot;user_id&quot;:2955,&quot;user_name&quot;:&quot;five&quot;,&quot;hello&quot;:&quot;ssm-seoul&quot;,&quot;uid&quot;:5}</td></tr>\n",
       "<tr><td>2954</td><td>{&quot;time&quot;:&quot;2022-09-30 17:40:31&quot;,&quot;user_id&quot;:2954,&quot;user_name&quot;:&quot;four&quot;,&quot;hello&quot;:&quot;ssm-seoul&quot;,&quot;uid&quot;:4}</td></tr>\n",
       "<tr><td>2953</td><td>{&quot;time&quot;:&quot;2022-09-30 17:40:30&quot;,&quot;user_id&quot;:2953,&quot;user_name&quot;:&quot;three&quot;,&quot;hello&quot;:&quot;ssm-seoul&quot;,&quot;uid&quot;:3}</td></tr>\n",
       "<tr><td>2952</td><td>{&quot;time&quot;:&quot;2022-09-30 17:40:29&quot;,&quot;user_id&quot;:2952,&quot;user_name&quot;:&quot;two&quot;,&quot;hello&quot;:&quot;ssm-seoul&quot;,&quot;uid&quot;:2}</td></tr>\n",
       "<tr><td>2951</td><td>{&quot;time&quot;:&quot;2022-09-30 17:40:28&quot;,&quot;user_id&quot;:2951,&quot;user_name&quot;:&quot;one&quot;,&quot;hello&quot;:&quot;ssm-seoul&quot;,&quot;uid&quot;:1}</td></tr>\n",
       "<tr><td>2950</td><td>{&quot;time&quot;:&quot;2022-09-30 17:40:27&quot;,&quot;user_id&quot;:2950,&quot;user_name&quot;:&quot;zero&quot;,&quot;hello&quot;:&quot;ssm-seoul&quot;,&quot;uid&quot;:0}</td></tr>\n",
       "<tr><td>2949</td><td>{&quot;time&quot;:&quot;2022-09-30 17:40:26&quot;,&quot;user_id&quot;:2949,&quot;hello&quot;:&quot;ssm-seoul&quot;}</td></tr>\n",
       "<tr><td>2948</td><td>{&quot;time&quot;:&quot;2022-09-30 17:40:25&quot;,&quot;user_id&quot;:2948,&quot;hello&quot;:&quot;ssm-seoul&quot;}</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----+---------------------------------------------------------------------------------------------+\n",
       "| key|                                                                                        value|\n",
       "+----+---------------------------------------------------------------------------------------------+\n",
       "|2959|                            {\"time\":\"2022-09-30 17:40:36\",\"user_id\":2959,\"hello\":\"ssm-seoul\"}|\n",
       "|2958|                            {\"time\":\"2022-09-30 17:40:35\",\"user_id\":2958,\"hello\":\"ssm-seoul\"}|\n",
       "|2957|                            {\"time\":\"2022-09-30 17:40:34\",\"user_id\":2957,\"hello\":\"ssm-seoul\"}|\n",
       "|2956|                            {\"time\":\"2022-09-30 17:40:33\",\"user_id\":2956,\"hello\":\"ssm-seoul\"}|\n",
       "|2955| {\"time\":\"2022-09-30 17:40:32\",\"user_id\":2955,\"user_name\":\"five\",\"hello\":\"ssm-seoul\",\"uid\":5}|\n",
       "|2954| {\"time\":\"2022-09-30 17:40:31\",\"user_id\":2954,\"user_name\":\"four\",\"hello\":\"ssm-seoul\",\"uid\":4}|\n",
       "|2953|{\"time\":\"2022-09-30 17:40:30\",\"user_id\":2953,\"user_name\":\"three\",\"hello\":\"ssm-seoul\",\"uid\":3}|\n",
       "|2952|  {\"time\":\"2022-09-30 17:40:29\",\"user_id\":2952,\"user_name\":\"two\",\"hello\":\"ssm-seoul\",\"uid\":2}|\n",
       "|2951|  {\"time\":\"2022-09-30 17:40:28\",\"user_id\":2951,\"user_name\":\"one\",\"hello\":\"ssm-seoul\",\"uid\":1}|\n",
       "|2950| {\"time\":\"2022-09-30 17:40:27\",\"user_id\":2950,\"user_name\":\"zero\",\"hello\":\"ssm-seoul\",\"uid\":0}|\n",
       "|2949|                            {\"time\":\"2022-09-30 17:40:26\",\"user_id\":2949,\"hello\":\"ssm-seoul\"}|\n",
       "|2948|                            {\"time\":\"2022-09-30 17:40:25\",\"user_id\":2948,\"hello\":\"ssm-seoul\"}|\n",
       "+----+---------------------------------------------------------------------------------------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "WriteToDataSourceV2 org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@38b40bf9\n",
      "+- Project [user_id#3466L AS key#3473L, to_json(struct(time, time#3122, user_id, user_id#3466L, user_name, user_name#3467, hello, hello#3121, uid, uid#3406L), Some(Asia/Seoul)) AS value#3474]\n",
      "   +- Project [time#3122, id#3120L AS user_id#3466L, name#3405 AS user_name#3467, hello#3121, uid#3406L]\n",
      "      +- Join LeftOuter, (mod_id#3119L = uid#3406L)\n",
      "         :- Project [(events#3115.id % cast(10 as bigint)) AS mod_id#3119L, events#3115.id AS id#3120L, events#3115.hello AS hello#3121, events#3115.time AS time#3122]\n",
      "         :  +- Project [cast(key#3101 as string) AS key#3116, from_json(StructField(id,LongType,true), StructField(hello,StringType,true), StructField(time,StringType,true), cast(value#3102 as string), Some(Asia/Seoul)) AS events#3115]\n",
      "         :     +- StreamingDataSourceV2Relation [key#3101, value#3102, topic#3103, partition#3104, offset#3105L, timestamp#3106, timestampType#3107], org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan@15309c31, KafkaV2[Subscribe[events]], {\"events\":{\"0\":2988}}, {\"events\":{\"0\":2996}}\n",
      "         +- Relation [name#3405,uid#3406L] json\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "WriteToDataSourceV2 org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@38b40bf9\n",
      "+- Project [user_id#3466L AS key#3473L, to_json(struct(time, time#3122, user_id, user_id#3466L, user_name, user_name#3467, hello, hello#3121, uid, uid#3406L), Some(Asia/Seoul)) AS value#3474]\n",
      "   +- Project [time#3122, id#3120L AS user_id#3466L, name#3405 AS user_name#3467, hello#3121, uid#3406L]\n",
      "      +- Join LeftOuter, (mod_id#3119L = uid#3406L)\n",
      "         :- Project [(events#3115.id % cast(10 as bigint)) AS mod_id#3119L, events#3115.id AS id#3120L, events#3115.hello AS hello#3121, events#3115.time AS time#3122]\n",
      "         :  +- Project [cast(key#3101 as string) AS key#3116, from_json(StructField(id,LongType,true), StructField(hello,StringType,true), StructField(time,StringType,true), cast(value#3102 as string), Some(Asia/Seoul)) AS events#3115]\n",
      "         :     +- StreamingDataSourceV2Relation [key#3101, value#3102, topic#3103, partition#3104, offset#3105L, timestamp#3106, timestampType#3107], org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan@15309c31, KafkaV2[Subscribe[events]], {\"events\":{\"0\":2988}}, {\"events\":{\"0\":2996}}\n",
      "         +- Relation [name#3405,uid#3406L] json\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "WriteToDataSourceV2 org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@38b40bf9\n",
      "+- Project [id#3120L AS key#3473L, to_json(struct(time, time#3122, user_id, id#3120L, user_name, name#3405, hello, hello#3121, uid, uid#3406L), Some(Asia/Seoul)) AS value#3474]\n",
      "   +- Join LeftOuter, (mod_id#3119L = uid#3406L)\n",
      "      :- Project [(from_json(StructField(id,LongType,true), cast(value#3102 as string), Some(Asia/Seoul)).id % 10) AS mod_id#3119L, from_json(StructField(id,LongType,true), cast(value#3102 as string), Some(Asia/Seoul)).id AS id#3120L, from_json(StructField(hello,StringType,true), cast(value#3102 as string), Some(Asia/Seoul)).hello AS hello#3121, from_json(StructField(time,StringType,true), cast(value#3102 as string), Some(Asia/Seoul)).time AS time#3122]\n",
      "      :  +- StreamingDataSourceV2Relation [key#3101, value#3102, topic#3103, partition#3104, offset#3105L, timestamp#3106, timestampType#3107], org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan@15309c31, KafkaV2[Subscribe[events]], {\"events\":{\"0\":2988}}, {\"events\":{\"0\":2996}}\n",
      "      +- Filter isnotnull(uid#3406L)\n",
      "         +- Relation [name#3405,uid#3406L] json\n",
      "\n",
      "== Physical Plan ==\n",
      "WriteToDataSourceV2 org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@38b40bf9, org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy$$Lambda$2573/0x000000084107e440@433d424c\n",
      "+- Project [id#3120L AS key#3473L, to_json(struct(time, time#3122, user_id, id#3120L, user_name, name#3405, hello, hello#3121, uid, uid#3406L), Some(Asia/Seoul)) AS value#3474]\n",
      "   +- *(2) BroadcastHashJoin [mod_id#3119L], [uid#3406L], LeftOuter, BuildRight, false\n",
      "      :- Project [(from_json(StructField(id,LongType,true), cast(value#3102 as string), Some(Asia/Seoul)).id % 10) AS mod_id#3119L, from_json(StructField(id,LongType,true), cast(value#3102 as string), Some(Asia/Seoul)).id AS id#3120L, from_json(StructField(hello,StringType,true), cast(value#3102 as string), Some(Asia/Seoul)).hello AS hello#3121, from_json(StructField(time,StringType,true), cast(value#3102 as string), Some(Asia/Seoul)).time AS time#3122]\n",
      "      :  +- MicroBatchScan[key#3101, value#3102, topic#3103, partition#3104, offset#3105L, timestamp#3106, timestampType#3107] class org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan\n",
      "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[1, bigint, false]),false), [id=#6999]\n",
      "         +- *(1) Filter isnotnull(uid#3406L)\n",
      "            +- FileScan json [name#3405,uid#3406L] Batched: false, DataFilters: [isnotnull(uid#3406L)], Format: JSON, Location: InMemoryFileIndex(1 paths)[file:/home/jovyan/work/data/names], PartitionFilters: [], PushedFilters: [IsNotNull(uid)], ReadSchema: struct<name:string,uid:bigint>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "staticQuery = staticTrigger.start()\n",
    "displayStream(queryName, f\"select * from {queryName} order by key desc\", 10, 6)\n",
    "staticQuery.explain(True)\n",
    "staticQuery.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f214bb5b-858c-4466-9a6c-cf9a95b3e41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "staticQuery.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5e955be3-87e2-4dbe-a19e-bf8c2fc61e68",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[kafkaSink] Iteration: 21, Status: Waiting for next trigger'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'id': '66571cc2-b231-4eab-9a22-faae530abc19',\n",
       " 'runId': '6ee2022a-4731-44c2-bcab-f3bfc2176d56',\n",
       " 'name': 'kafkaSink',\n",
       " 'timestamp': '2022-09-30T08:53:35.000Z',\n",
       " 'batchId': 39,\n",
       " 'numInputRows': 4,\n",
       " 'inputRowsPerSecond': 0.8001600320064013,\n",
       " 'processedRowsPerSecond': 5.649717514124294,\n",
       " 'durationMs': {'addBatch': 584,\n",
       "  'getBatch': 0,\n",
       "  'latestOffset': 2,\n",
       "  'queryPlanning': 7,\n",
       "  'triggerExecution': 708,\n",
       "  'walCommit': 60},\n",
       " 'stateOperators': [],\n",
       " 'sources': [{'description': 'KafkaV2[Subscribe[events]]',\n",
       "   'startOffset': {'events': {'0': 3760}},\n",
       "   'endOffset': {'events': {'0': 3764}},\n",
       "   'latestOffset': {'events': {'0': 3764}},\n",
       "   'numInputRows': 4,\n",
       "   'inputRowsPerSecond': 0.8001600320064013,\n",
       "   'processedRowsPerSecond': 5.649717514124294,\n",
       "   'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
       "    'maxOffsetsBehindLatest': '0',\n",
       "    'minOffsetsBehindLatest': '0'}}],\n",
       " 'sink': {'description': 'org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@2a913d0c',\n",
       "  'numOutputRows': 4}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [63]\u001b[0m, in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m staticTrigger \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     18\u001b[0m     staticWriter\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;241m.\u001b[39mtrigger(processingTime\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m5 second\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpointLocation\u001b[39m\u001b[38;5;124m\"\u001b[39m, checkpointLocation)\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     23\u001b[0m staticQuery \u001b[38;5;241m=\u001b[39m staticTrigger\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m---> 24\u001b[0m \u001b[43mdisplayStatus\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueryName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstaticQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m staticQuery\u001b[38;5;241m.\u001b[39mstop()\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mdisplayStatus\u001b[0;34m(name, query, iterations, sleep_secs)\u001b[0m\n\u001b[1;32m     18\u001b[0m display(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m] Iteration: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(i)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, Status: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mquery\u001b[38;5;241m.\u001b[39mstatus[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     19\u001b[0m display(query\u001b[38;5;241m.\u001b[39mlastProgress)  \u001b[38;5;66;03m# 마지막 수행된 쿼리의 상태를 출력합니다\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43msleep_secs\u001b[49m\u001b[43m)\u001b[49m            \u001b[38;5;66;03m# 지정된 시간(초)을 대기합니다\u001b[39;00m\n\u001b[1;32m     21\u001b[0m i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "queryName = \"kafkaSink\"\n",
    "staticWriter = (\n",
    "    staticSelector\n",
    "    .selectExpr(\"time\", \"id as user_id\", \"name as user_name\", \"hello\", \"uid\")\n",
    "    .selectExpr(\"cast(user_id as string) as key\", \"to_json(struct(*)) as value\")\n",
    "    .writeStream\n",
    "    .queryName(queryName)\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9093\")\n",
    "    .option(\"topic\", \"events_enrich\")\n",
    "    .outputMode(\"append\")\n",
    ")\n",
    "\n",
    "checkpointLocation = f\"{work_dir}/tmp/{queryName}\"\n",
    "!rm -rf $checkpointLocation\n",
    "\n",
    "staticTrigger = (\n",
    "    staticWriter\n",
    "    .trigger(processingTime=\"5 second\")\n",
    "    .option(\"checkpointLocation\", checkpointLocation)\n",
    ")\n",
    "\n",
    "staticQuery = staticTrigger.start()\n",
    "displayStatus(queryName, staticQuery, 1000, 10)\n",
    "staticQuery.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19535092-4901-4339-b2be-a6b968db3446",
   "metadata": {},
   "outputs": [],
   "source": [
    "json = spark.read.json(\"/source/json\")\n",
    "\n",
    "# API 활용하여 조회하는 방법\n",
    "json.select(\"emp_id\", \"emp_name\")\n",
    "\n",
    "# SQL 활용하여 조회하는 방법\n",
    "json.createOrReplaceTempView(\"simple\")\n",
    "spark.sql(\"select emp_id, emp_name from simple\")\n",
    "\n",
    "# 데이터 출력\n",
    "json.show()\n",
    "\n",
    "# 경로에 저장\n",
    "json.write.json(\"/target/json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
