# 카파 아키텍처 실습
> 벌크 색인 대신에 모든 데이터를 스트리밍 형태로 유지하고, 스트림 파이프라인을 데이터 저장소로 인지하고 이를 통해 기본적인 데이터 처리 및 재처리를 전재하고 있기 때문에 서빙 레이어에서 `MySQL 혹은 MongoDB`같은 동적 업데이트를 지원하는 엔진이 적합합니다. 혹은 스트림 소스를 그대로 노출하고 컨슈머를 통해 도메인 별로 애플리케이션을 구성하는 `Data Mesh` 아키텍처로 접근하기도 합니다


## I. 카파 아키텍처 스트리밍 데이터 데이터 Stage & Aggregation 실습
> 카파 아키텍처의 기본 스트리밍 파이프라인을 구성하여, 데이터를 스테이징 한 이후에, 해당 토픽을 구성하고 데이터 처리 파이프라인을 구성합니다
* stream pipeline : fluentd -> kafka -> spark-stream-stage -> kafka -> spark-stream-agg-small -> mysql -> phpmyadmin

### 1. `fluentd` 통한 예제 데이터 생성
> fluentd dummy plugin 을 통해 `{id:number, time:timestamp}` 형식의 예제 데이터 생성

### 2. `kafka` 클러스터에 `events` 토픽에 저장된 데이터 확인
> `kafka-console-consumer`를 통해 저장된 토픽 데이터 확인

### 3. `kafka` 클러스터에 `names` 글로벌 토픽에 저장된 데이터 조인
> 카프카의 글로벌 테이블인 `names`에 0~5번 id 값에 매칭되는 값을 생성하여 저장

### 4. `notebook` 을 통해 `spark streaming` 통해 `events_stage` 토픽에 데이터 저장
> 스파크 스트리밍을 통해 `events` 토픽의 `id` 와 `names` 테이블의 `id, id_name` 컬럼을 조인하여 그대로 `events_stage` 토픽에 저장
> 집계된 데이터를 `events_stage`에 잘 저장되었는지 확인하고, 이 토픽이 스테이징된 데이터 원본으로 활용될 테이블(토픽)입니다 

### 5. `notebook` 을 통해 생성된 `events_stage` 토픽을 `spark-stream` 통하여 집계된 데이터를 콘솔에 출력
> 통하여 데이터 스캔 및 집계 지표 확인할 수 있습니다.
> 이 때에 스트리밍 조인 및 집계 처리시에는 반드시 윈도우 함수가 필요하며, 제한된 윈도우 크기를 통해 수행되어야만 합니다 (trade-off)

### 6. `spark-stream` 통계 집계된 데이터를 `mysql` 싱크를 통해 저장합니다
> 적재된 데이터가 `phpmyadmin` 통하여 잘 조회 되는지 확인합니다


## II. 카파 아키텍처 스트리밍 데이터 Recovery 실습
> 이미 생성된 stage 를 원본 스트림 소스로 활용하여 스트리밍 애플리케이션만 다시 수행하므로써 복구가 가능합니다
* backup pipeline : spark-stream-stage -> mysql

### 1. `kafka` 에 저장된 `events` 토픽을 활용하여 지연된 로그 및 변경된 정보를 반영합니다
> 글로벌 테이블에 추가된 메타데이터를 `names` 테이블에 반영합니다

### 2. 기존 스트리밍 애플리케이션을 설정만 변경하여 재수행합니다
> 기존 코드를 그대로 활용하되 windows 크기와 resource 확장하여 별도의 파이프라인을 실행합니다
> `events_stage` 토픽을 읽되 긴 지연시간과, 많은 리소스를 활용하여 `mysql`에 집계 지표를 캐치업합니다


## III. 카파 아키텍처의 확장
> 정재 및 확장된 스테이징 데이터소스가 존재하기 때문에 이를 활용하여 추가적인 스트림 파이프라인을 손쉽게 구성할 수 있습니다

### 1. 원하는 수준까지 정재된 데이터 토픽을 선택하고 이를 통해 특정 도메인의 실시간 지표를 제공할 수 있습니다
> 예를 들어 실시간으로 최근 30분 이내에 발생하는 모든 이름을 보여주는 대시보드를 생성하려고 한다면 

### 2. `events_stage` 토픽을 활용하여 `spark streaming` 애플리케이션을 통한 집계를 출력합니다
> 이 때에 데이터에 대한 전처리는 끝났고, 순수한 집계 쿼리만 작성하기만 하면 됩니다

### 3. 생성된 키워드만 `mongodb` 와 같은 해당 부서에서 사용하고자 하는 싱크에 저장합니다
> `mongo-express` 같은 도구를 통해 확인이 가능합니다

