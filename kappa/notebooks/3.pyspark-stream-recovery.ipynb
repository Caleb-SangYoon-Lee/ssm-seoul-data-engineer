{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d18f2b39-5f97-41d0-9c9a-31ec0b7a2bda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://bc7df2c387a2:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f57552966a0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from IPython.display import display, display_pretty, clear_output, JSON\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.sql.session.timeZone\", \"Asia/Seoul\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# 노트북에서 테이블 형태로 데이터 프레임 출력을 위한 설정을 합니다\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # display enabled\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.truncate\", 100) # display output columns size\n",
    "\n",
    "# 공통 데이터 위치\n",
    "home_jovyan = \"/home/jovyan\"\n",
    "work_data = f\"{home_jovyan}/work/data\"\n",
    "work_dir=!pwd\n",
    "work_dir = work_dir[0]\n",
    "\n",
    "# 로컬 환경 최적화\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5) # the number of partitions to use when shuffling data for joins or aggregations.\n",
    "spark.conf.set(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\")\n",
    "\n",
    "# 현재 기동된 스파크 애플리케이션의 포트를 확인하기 위해 스파크 정보를 출력합니다\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b0e4035-38f0-474e-8000-faf2d4c4802d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스트림 테이블을 주기적으로 조회하는 함수 (name: 이름, sql: Spark SQL, iterations: 반복횟수, sleep_secs: 인터벌)\n",
    "def displayStream(name, sql, iterations, sleep_secs):\n",
    "    from time import sleep\n",
    "    i = 1\n",
    "    for x in range(iterations):\n",
    "        clear_output(wait=True)              # 출력 Cell 을 지웁니다\n",
    "        display('[' + name + '] Iteration: '+str(i)+', Query: '+sql)\n",
    "        display(spark.sql(sql))              # Spark SQL 을 수행합니다\n",
    "        sleep(sleep_secs)                    # sleep_secs 초 만큼 대기합니다\n",
    "        i += 1\n",
    "\n",
    "# 스트림 쿼리의 상태를 주기적으로 조회하는 함수 (name: 이름, query: Streaming Query, iterations: 반복횟수, sleep_secs: 인터벌)\n",
    "def displayStatus(name, query, iterations, sleep_secs):\n",
    "    from time import sleep\n",
    "    i = 1\n",
    "    for x in range(iterations):\n",
    "        clear_output(wait=True)      # Output Cell 의 내용을 지웁니다\n",
    "        display('[' + name + '] Iteration: '+str(i)+', Status: '+query.status['message'])\n",
    "        display(query.lastProgress)  # 마지막 수행된 쿼리의 상태를 출력합니다\n",
    "        sleep(sleep_secs)            # 지정된 시간(초)을 대기합니다\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37d8b8bc-13c5-43a5-81f2-c8307e44582c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- time: string (nullable = true)\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- user_name: string (nullable = true)\n",
      " |-- hello: string (nullable = true)\n",
      " |-- uid: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "source_topic = \"events_stage\"\n",
    "source_offset = \"latest\"\n",
    "query_name = \"jdbc_sink\"\n",
    "sink_table = \"events_agg\"\n",
    "\n",
    "kafkaReader = (\n",
    "    spark\n",
    "  .readStream\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka:9093\")\n",
    "  .option(\"subscribe\", source_topic)\n",
    "  .option(\"startingOffsets\", source_offset)\n",
    "  .load()\n",
    ")\n",
    "kafkaReader.printSchema()\n",
    "\n",
    "# {\"time\":\"2022-10-01 07:41:26\",\"user_id\":354,\"user_name\":\"four\",\"hello\":\"ssm-seoul\",\"uid\":4}\n",
    "kafkaSchema = (\n",
    "    StructType()\n",
    "    .add(StructField(\"time\", StringType()))\n",
    "    .add(StructField(\"user_id\", LongType()))\n",
    "    .add(StructField(\"user_name\", StringType()))\n",
    "    .add(StructField(\"hello\", StringType()))\n",
    "    .add(StructField(\"uid\", LongType()))\n",
    ")\n",
    "\n",
    "kafkaSelector = (\n",
    "    kafkaReader\n",
    "    .select(\n",
    "        col(\"key\").cast(\"string\"),\n",
    "        from_json(col(\"value\").cast(\"string\"), kafkaSchema).alias(\"stage\")\n",
    "    )\n",
    "    .selectExpr(\"stage.*\")\n",
    ")\n",
    "\n",
    "kafkaSelector.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "87ce2c8f-aae8-42ec-b09e-0f5e946d28a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_name \"memorySink\"\n",
    "# memoryWriter = (\n",
    "#     kafkaSelector\n",
    "#     .selectExpr(\"to_timestamp(time) as timestamp\", \"user_name\")\n",
    "#     .withWatermark(\"timestamp\", \"1 minute\")\n",
    "#     .groupBy(\"user_name\", window(\"timestamp\", \"1 minute\", \"1 minute\")).count().alias(\"count\")\n",
    "#     .writeStream\n",
    "#     .queryName(query_name)\n",
    "#     .format(\"memory\")\n",
    "#     .outputMode(\"complete\")\n",
    "# )\n",
    "\n",
    "# checkpointLocation = f\"{work_dir}/tmp/{query_name}\"\n",
    "# !rm -rf $checkpointLocation\n",
    "\n",
    "# memoryTrigger = (\n",
    "#     memoryWriter\n",
    "#     .trigger(processingTime=\"5 second\")\n",
    "#     .option(\"checkpointLocation\", checkpointLocation)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c0f458e4-e856-42a5-b62f-3e28b737eb51",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[memorySink] Iteration: 10, Query: select * from memorySink order by window asc'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>user_name</th><th>window</th><th>count</th></tr>\n",
       "<tr><td>six</td><td>{2022-10-01 08:21:00, 2022-10-01 08:22:00}</td><td>1</td></tr>\n",
       "<tr><td>five</td><td>{2022-10-01 08:21:00, 2022-10-01 08:22:00}</td><td>1</td></tr>\n",
       "<tr><td>four</td><td>{2022-10-01 08:21:00, 2022-10-01 08:22:00}</td><td>1</td></tr>\n",
       "<tr><td>two</td><td>{2022-10-01 08:21:00, 2022-10-01 08:22:00}</td><td>1</td></tr>\n",
       "<tr><td>seven</td><td>{2022-10-01 08:21:00, 2022-10-01 08:22:00}</td><td>1</td></tr>\n",
       "<tr><td>three</td><td>{2022-10-01 08:21:00, 2022-10-01 08:22:00}</td><td>1</td></tr>\n",
       "<tr><td>three</td><td>{2022-10-01 08:22:00, 2022-10-01 08:23:00}</td><td>4</td></tr>\n",
       "<tr><td>seven</td><td>{2022-10-01 08:22:00, 2022-10-01 08:23:00}</td><td>4</td></tr>\n",
       "<tr><td>five</td><td>{2022-10-01 08:22:00, 2022-10-01 08:23:00}</td><td>4</td></tr>\n",
       "<tr><td>four</td><td>{2022-10-01 08:22:00, 2022-10-01 08:23:00}</td><td>4</td></tr>\n",
       "<tr><td>eight</td><td>{2022-10-01 08:22:00, 2022-10-01 08:23:00}</td><td>5</td></tr>\n",
       "<tr><td>nine</td><td>{2022-10-01 08:22:00, 2022-10-01 08:23:00}</td><td>5</td></tr>\n",
       "<tr><td>two</td><td>{2022-10-01 08:22:00, 2022-10-01 08:23:00}</td><td>4</td></tr>\n",
       "<tr><td>zero</td><td>{2022-10-01 08:22:00, 2022-10-01 08:23:00}</td><td>4</td></tr>\n",
       "<tr><td>one</td><td>{2022-10-01 08:22:00, 2022-10-01 08:23:00}</td><td>4</td></tr>\n",
       "<tr><td>six</td><td>{2022-10-01 08:22:00, 2022-10-01 08:23:00}</td><td>4</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---------+------------------------------------------+-----+\n",
       "|user_name|                                    window|count|\n",
       "+---------+------------------------------------------+-----+\n",
       "|    three|{2022-10-01 08:21:00, 2022-10-01 08:22:00}|    1|\n",
       "|     five|{2022-10-01 08:21:00, 2022-10-01 08:22:00}|    1|\n",
       "|      two|{2022-10-01 08:21:00, 2022-10-01 08:22:00}|    1|\n",
       "|      six|{2022-10-01 08:21:00, 2022-10-01 08:22:00}|    1|\n",
       "|    seven|{2022-10-01 08:21:00, 2022-10-01 08:22:00}|    1|\n",
       "|     four|{2022-10-01 08:21:00, 2022-10-01 08:22:00}|    1|\n",
       "|    eight|{2022-10-01 08:22:00, 2022-10-01 08:23:00}|    5|\n",
       "|     four|{2022-10-01 08:22:00, 2022-10-01 08:23:00}|    4|\n",
       "|      one|{2022-10-01 08:22:00, 2022-10-01 08:23:00}|    4|\n",
       "|      two|{2022-10-01 08:22:00, 2022-10-01 08:23:00}|    4|\n",
       "|      six|{2022-10-01 08:22:00, 2022-10-01 08:23:00}|    4|\n",
       "|    seven|{2022-10-01 08:22:00, 2022-10-01 08:23:00}|    4|\n",
       "|     nine|{2022-10-01 08:22:00, 2022-10-01 08:23:00}|    5|\n",
       "|     zero|{2022-10-01 08:22:00, 2022-10-01 08:23:00}|    4|\n",
       "|    three|{2022-10-01 08:22:00, 2022-10-01 08:23:00}|    4|\n",
       "|     five|{2022-10-01 08:22:00, 2022-10-01 08:23:00}|    4|\n",
       "+---------+------------------------------------------+-----+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "WriteToDataSourceV2 org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@30abc25e\n",
      "+- SubqueryAlias count\n",
      "   +- Aggregate [user_name#27, window#17933-T60000ms], [user_name#27, window#17933-T60000ms AS window#17928-T60000ms, count(1) AS count#17932L]\n",
      "      +- Filter isnotnull(timestamp#17925-T60000ms)\n",
      "         +- Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#17925-T60000ms, TimestampType, LongType) - 0) as double) / cast(60000000 as double))) as double) = (cast((precisetimestampconversion(timestamp#17925-T60000ms, TimestampType, LongType) - 0) as double) / cast(60000000 as double))) THEN (CEIL((cast((precisetimestampconversion(timestamp#17925-T60000ms, TimestampType, LongType) - 0) as double) / cast(60000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(timestamp#17925-T60000ms, TimestampType, LongType) - 0) as double) / cast(60000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 60000000) + 0), LongType, TimestampType), end, precisetimestampconversion((((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#17925-T60000ms, TimestampType, LongType) - 0) as double) / cast(60000000 as double))) as double) = (cast((precisetimestampconversion(timestamp#17925-T60000ms, TimestampType, LongType) - 0) as double) / cast(60000000 as double))) THEN (CEIL((cast((precisetimestampconversion(timestamp#17925-T60000ms, TimestampType, LongType) - 0) as double) / cast(60000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(timestamp#17925-T60000ms, TimestampType, LongType) - 0) as double) / cast(60000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 60000000) + 0) + 60000000), LongType, TimestampType)) AS window#17933-T60000ms, timestamp#17925-T60000ms, user_name#27]\n",
      "            +- EventTimeWatermark timestamp#17925: timestamp, 1 minutes\n",
      "               +- Project [to_timestamp(time#25, None) AS timestamp#17925, user_name#27]\n",
      "                  +- Project [stage#21.time AS time#25, stage#21.user_id AS user_id#26L, stage#21.user_name AS user_name#27, stage#21.hello AS hello#28, stage#21.uid AS uid#29L]\n",
      "                     +- Project [cast(key#7 as string) AS key#22, from_json(StructField(time,StringType,true), StructField(user_id,LongType,true), StructField(user_name,StringType,true), StructField(hello,StringType,true), StructField(uid,LongType,true), cast(value#8 as string), Some(Asia/Seoul)) AS stage#21]\n",
      "                        +- StreamingDataSourceV2Relation [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan@2f9bd5e8, KafkaV2[Subscribe[events_stage]], {\"events_stage\":{\"0\":2554}}, {\"events_stage\":{\"0\":2558}}\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "WriteToDataSourceV2 org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@30abc25e\n",
      "+- SubqueryAlias count\n",
      "   +- Aggregate [user_name#27, window#17933-T60000ms], [user_name#27, window#17933-T60000ms AS window#17928-T60000ms, count(1) AS count#17932L]\n",
      "      +- Filter isnotnull(timestamp#17925-T60000ms)\n",
      "         +- Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#17925-T60000ms, TimestampType, LongType) - 0) as double) / cast(60000000 as double))) as double) = (cast((precisetimestampconversion(timestamp#17925-T60000ms, TimestampType, LongType) - 0) as double) / cast(60000000 as double))) THEN (CEIL((cast((precisetimestampconversion(timestamp#17925-T60000ms, TimestampType, LongType) - 0) as double) / cast(60000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(timestamp#17925-T60000ms, TimestampType, LongType) - 0) as double) / cast(60000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 60000000) + 0), LongType, TimestampType), end, precisetimestampconversion((((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#17925-T60000ms, TimestampType, LongType) - 0) as double) / cast(60000000 as double))) as double) = (cast((precisetimestampconversion(timestamp#17925-T60000ms, TimestampType, LongType) - 0) as double) / cast(60000000 as double))) THEN (CEIL((cast((precisetimestampconversion(timestamp#17925-T60000ms, TimestampType, LongType) - 0) as double) / cast(60000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(timestamp#17925-T60000ms, TimestampType, LongType) - 0) as double) / cast(60000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 60000000) + 0) + 60000000), LongType, TimestampType)) AS window#17933-T60000ms, timestamp#17925-T60000ms, user_name#27]\n",
      "            +- EventTimeWatermark timestamp#17925: timestamp, 1 minutes\n",
      "               +- Project [to_timestamp(time#25, None) AS timestamp#17925, user_name#27]\n",
      "                  +- Project [stage#21.time AS time#25, stage#21.user_id AS user_id#26L, stage#21.user_name AS user_name#27, stage#21.hello AS hello#28, stage#21.uid AS uid#29L]\n",
      "                     +- Project [cast(key#7 as string) AS key#22, from_json(StructField(time,StringType,true), StructField(user_id,LongType,true), StructField(user_name,StringType,true), StructField(hello,StringType,true), StructField(uid,LongType,true), cast(value#8 as string), Some(Asia/Seoul)) AS stage#21]\n",
      "                        +- StreamingDataSourceV2Relation [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan@2f9bd5e8, KafkaV2[Subscribe[events_stage]], {\"events_stage\":{\"0\":2554}}, {\"events_stage\":{\"0\":2558}}\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "WriteToDataSourceV2 org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@30abc25e\n",
      "+- Aggregate [user_name#27, window#17933-T60000ms], [user_name#27, window#17933-T60000ms, count(1) AS count#17932L]\n",
      "   +- Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#17925-T60000ms, TimestampType, LongType) - 0) as double) / 6.0E7)) as double) = (cast((precisetimestampconversion(timestamp#17925-T60000ms, TimestampType, LongType) - 0) as double) / 6.0E7)) THEN (CEIL((cast((precisetimestampconversion(timestamp#17925-T60000ms, TimestampType, LongType) - 0) as double) / 6.0E7)) + 1) ELSE CEIL((cast((precisetimestampconversion(timestamp#17925-T60000ms, TimestampType, LongType) - 0) as double) / 6.0E7)) END + 0) - 1) * 60000000) + 0), LongType, TimestampType), end, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#17925-T60000ms, TimestampType, LongType) - 0) as double) / 6.0E7)) as double) = (cast((precisetimestampconversion(timestamp#17925-T60000ms, TimestampType, LongType) - 0) as double) / 6.0E7)) THEN (CEIL((cast((precisetimestampconversion(timestamp#17925-T60000ms, TimestampType, LongType) - 0) as double) / 6.0E7)) + 1) ELSE CEIL((cast((precisetimestampconversion(timestamp#17925-T60000ms, TimestampType, LongType) - 0) as double) / 6.0E7)) END + 0) - 1) * 60000000) + 60000000), LongType, TimestampType)) AS window#17933-T60000ms, user_name#27]\n",
      "      +- Filter isnotnull(timestamp#17925-T60000ms)\n",
      "         +- EventTimeWatermark timestamp#17925: timestamp, 1 minutes\n",
      "            +- Project [cast(from_json(StructField(time,StringType,true), cast(value#8 as string), Some(Asia/Seoul)).time as timestamp) AS timestamp#17925, from_json(StructField(user_name,StringType,true), cast(value#8 as string), Some(Asia/Seoul)).user_name AS user_name#27]\n",
      "               +- StreamingDataSourceV2Relation [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan@2f9bd5e8, KafkaV2[Subscribe[events_stage]], {\"events_stage\":{\"0\":2554}}, {\"events_stage\":{\"0\":2558}}\n",
      "\n",
      "== Physical Plan ==\n",
      "WriteToDataSourceV2 org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@30abc25e, org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy$$Lambda$2473/0x0000000840fd8040@2b56dac4\n",
      "+- *(4) HashAggregate(keys=[user_name#27, window#17933-T60000ms], functions=[count(1)], output=[user_name#27, window#17933-T60000ms, count#17932L])\n",
      "   +- StateStoreSave [user_name#27, window#17933-T60000ms], state info [ checkpoint = file:/home/jovyan/work/tmp/memorySink/state, runId = 90de03f6-6773-43c0-b556-a18ba9124c67, opId = 0, ver = 11, numPartitions = 5], Complete, 1664580101000, 2\n",
      "      +- *(3) HashAggregate(keys=[user_name#27, window#17933-T60000ms], functions=[merge_count(1)], output=[user_name#27, window#17933-T60000ms, count#17947L])\n",
      "         +- StateStoreRestore [user_name#27, window#17933-T60000ms], state info [ checkpoint = file:/home/jovyan/work/tmp/memorySink/state, runId = 90de03f6-6773-43c0-b556-a18ba9124c67, opId = 0, ver = 11, numPartitions = 5], 2\n",
      "            +- *(2) HashAggregate(keys=[user_name#27, window#17933-T60000ms], functions=[merge_count(1)], output=[user_name#27, window#17933-T60000ms, count#17947L])\n",
      "               +- Exchange hashpartitioning(user_name#27, window#17933-T60000ms, 5), ENSURE_REQUIREMENTS, [id=#53295]\n",
      "                  +- *(1) HashAggregate(keys=[user_name#27, window#17933-T60000ms], functions=[partial_count(1)], output=[user_name#27, window#17933-T60000ms, count#17947L])\n",
      "                     +- *(1) Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#17925-T60000ms, TimestampType, LongType) - 0) as double) / 6.0E7)) as double) = (cast((precisetimestampconversion(timestamp#17925-T60000ms, TimestampType, LongType) - 0) as double) / 6.0E7)) THEN (CEIL((cast((precisetimestampconversion(timestamp#17925-T60000ms, TimestampType, LongType) - 0) as double) / 6.0E7)) + 1) ELSE CEIL((cast((precisetimestampconversion(timestamp#17925-T60000ms, TimestampType, LongType) - 0) as double) / 6.0E7)) END + 0) - 1) * 60000000) + 0), LongType, TimestampType), end, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(timestamp#17925-T60000ms, TimestampType, LongType) - 0) as double) / 6.0E7)) as double) = (cast((precisetimestampconversion(timestamp#17925-T60000ms, TimestampType, LongType) - 0) as double) / 6.0E7)) THEN (CEIL((cast((precisetimestampconversion(timestamp#17925-T60000ms, TimestampType, LongType) - 0) as double) / 6.0E7)) + 1) ELSE CEIL((cast((precisetimestampconversion(timestamp#17925-T60000ms, TimestampType, LongType) - 0) as double) / 6.0E7)) END + 0) - 1) * 60000000) + 60000000), LongType, TimestampType)) AS window#17933-T60000ms, user_name#27]\n",
      "                        +- *(1) Filter isnotnull(timestamp#17925-T60000ms)\n",
      "                           +- EventTimeWatermark timestamp#17925: timestamp, 1 minutes\n",
      "                              +- Project [cast(from_json(StructField(time,StringType,true), cast(value#8 as string), Some(Asia/Seoul)).time as timestamp) AS timestamp#17925, from_json(StructField(user_name,StringType,true), cast(value#8 as string), Some(Asia/Seoul)).user_name AS user_name#27]\n",
      "                                 +- MicroBatchScan[key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13] class org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# memoryQuery = memoryTrigger.start()\n",
    "# displayStream(query_name, f\"select * from {query_name} order by window asc\", 10, 6)\n",
    "# memoryQuery.explain(True)\n",
    "# memoryQuery.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e63a5d50-1235-43b6-9107-99adf50e8c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf $checkpointLocation\n",
    "# memoryQuery.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b13a213e-ca3d-4950-baaa-e6ec4124c624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# append mode 동작 방식에 대한 이해\n",
    "\n",
    "def saveToMySql(dataFrame, batchId):\n",
    "    url = \"jdbc:mysql://scott:tiger@mysql:3306/default?useUnicode=true&serverTimezone=Asia/Seoul\"\n",
    "    writer = (\n",
    "        dataFrame\n",
    "        .write\n",
    "        .format(\"jdbc\")\n",
    "        .option(\"url\", url)\n",
    "        .option(\"dbtable\", sink_table)\n",
    "        .mode(\"append\")\n",
    "    )\n",
    "    writer.save()\n",
    "\n",
    "jdbcWriter = (\n",
    "    kafkaSelector\n",
    "    .selectExpr(\"to_timestamp(time) as timestamp\", \"user_name\")\n",
    "    .withWatermark(\"timestamp\", \"1 minute\")\n",
    "    .groupBy(\"user_name\", window(\"timestamp\", \"1 minute\", \"1 minute\")).count().alias(\"count\")\n",
    "    .select(\"window.start\", \"window.end\", \"count\")\n",
    "    .writeStream\n",
    "    .queryName(query_name)\n",
    "    .foreachBatch(saveToMySql)\n",
    ")\n",
    "\n",
    "checkpointLocation = f\"{work_dir}/tmp/{query_name}\"\n",
    "!rm -rf $checkpointLocation\n",
    "\n",
    "jdbcTrigger = (\n",
    "    jdbcWriter\n",
    "    .trigger(processingTime=\"5 second\")\n",
    "    .option(\"checkpointLocation\", checkpointLocation)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7e32d1cc-7b97-435b-80a1-02b9d7c35101",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[jdbcSink] Iteration: 100, Status: Waiting for next trigger'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'id': '96307546-3b66-4b6b-acaf-44f338f7cc18',\n",
       " 'runId': 'cda9b1bf-3dbd-47db-ae83-2d73061888a8',\n",
       " 'name': 'jdbcSink',\n",
       " 'timestamp': '2022-09-30T23:49:15.000Z',\n",
       " 'batchId': 59,\n",
       " 'numInputRows': 8,\n",
       " 'inputRowsPerSecond': 1.6,\n",
       " 'processedRowsPerSecond': 9.25925925925926,\n",
       " 'durationMs': {'addBatch': 752,\n",
       "  'getBatch': 0,\n",
       "  'latestOffset': 1,\n",
       "  'queryPlanning': 11,\n",
       "  'triggerExecution': 864,\n",
       "  'walCommit': 54},\n",
       " 'eventTime': {'avg': '2022-09-30T23:49:04.500Z',\n",
       "  'max': '2022-09-30T23:49:08.000Z',\n",
       "  'min': '2022-09-30T23:49:01.000Z',\n",
       "  'watermark': '2022-09-30T23:48:00.000Z'},\n",
       " 'stateOperators': [{'operatorName': 'stateStoreSave',\n",
       "   'numRowsTotal': 19,\n",
       "   'numRowsUpdated': 8,\n",
       "   'allUpdatesTimeMs': 134,\n",
       "   'numRowsRemoved': 10,\n",
       "   'allRemovalsTimeMs': 4,\n",
       "   'commitTimeMs': 258,\n",
       "   'memoryUsedBytes': 9368,\n",
       "   'numRowsDroppedByWatermark': 0,\n",
       "   'numShufflePartitions': 5,\n",
       "   'numStateStoreInstances': 5,\n",
       "   'customMetrics': {'loadedMapCacheHitCount': 590,\n",
       "    'loadedMapCacheMissCount': 0,\n",
       "    'stateOnCurrentVersionSizeBytes': 4992}}],\n",
       " 'sources': [{'description': 'KafkaV2[Subscribe[events_stage]]',\n",
       "   'startOffset': {'events_stage': {'0': 4133}},\n",
       "   'endOffset': {'events_stage': {'0': 4141}},\n",
       "   'latestOffset': {'events_stage': {'0': 4141}},\n",
       "   'numInputRows': 8,\n",
       "   'inputRowsPerSecond': 1.6,\n",
       "   'processedRowsPerSecond': 9.25925925925926,\n",
       "   'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
       "    'maxOffsetsBehindLatest': '0',\n",
       "    'minOffsetsBehindLatest': '0'}}],\n",
       " 'sink': {'description': 'ForeachBatchSink', 'numOutputRows': -1}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jdbcQuery = jdbcTrigger.start()\n",
    "displayStatus(query_name, jdbcQuery, 100, 6)\n",
    "jdbcQuery.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e93533f-d8b5-457b-95c4-de568f6c4051",
   "metadata": {
    "tags": []
   },
   "source": [
    "'[jdbcSink] Iteration: 33, Status: Terminated with exception: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\\n  File \"/opt/conda/lib/python3.9/site-packages/py4j/clientserver.py\", line 581, in _call_proxy\\n    return_value = getattr(self.pool[obj_id], method)(*params)\\n  File \"/opt/conda/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 196, in call\\n    raise e\\n  File \"/opt/conda/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 193, in call\\n    self.func(DataFrame(jdf, self.sql_ctx), batch_id)\\n  File \"/tmp/ipykernel_3274/1984705217.py\", line 14, in saveToMySql\\n    writer.save()\\n  File \"/opt/conda/lib/python3.9/site-packages/pyspark/sql/readwriter.py\", line 738, in save\\n    self._jwrite.save()\\n  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\\n    return_value = get_return_value(\\n  File \"/opt/conda/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 111, in deco\\n    return f(*a, **kw)\\n  File \"/opt/conda/lib/python3.9/site-packages/py4j/protocol.py\", line 326, in get_return_value\\n    raise Py4JJavaError(\\npy4j.protocol.Py4JJavaError: An error occurred while calling o1101.save.\\n: java.sql.SQLException: The server time zone value \\'KST\\' is unrecognized or represents more than one time zone. You must configure either the server or JDBC driver (via the \\'serverTimezone\\' configuration property) to use a more specifc time zone value if you want to utilize time zone support.\\n\\tat com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:129)\\n\\tat com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:97)\\n\\tat com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:89)\\n\\tat com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:63)\\n\\tat com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:73)\\n\\tat com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:76)\\n\\tat com.mysql.cj.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:836)\\n\\tat com.mysql.cj.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:456)\\n\\tat com.mysql.cj.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:246)\\n\\tat com.mysql.cj.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:197)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProvider$.create(ConnectionProvider.scala:77)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$createConnectionFactory$1(JdbcUtils.scala:64)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:49)\\n\\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\\n\\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\\n\\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\\n\\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\\n\\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\\n\\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)\\n\\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\\n\\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\\n\\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)\\n\\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)\\n\\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:128)\\n\\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\\n\\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\\n\\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\\n\\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\\n\\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\\n\\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\\n\\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\\n\\tat com.sun.proxy.$Proxy22.call(Unknown Source)\\n\\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:55)\\n\\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:55)\\n\\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:35)\\n\\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:600)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\\n\\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:598)\\n\\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\\n\\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\\n\\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\\n\\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:598)\\n\\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:228)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\\n\\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\\n\\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\\n\\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\\n\\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:193)\\n\\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)\\n\\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:187)\\n\\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:303)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\\n\\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:286)\\n\\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:209)\\nCaused by: com.mysql.cj.exceptions.InvalidConnectionAttributeException: The server time zone value \\'KST\\' is unrecognized or represents more than one time zone. You must configure either the server or JDBC driver (via the \\'serverTimezone\\' configuration property) to use a more specifc time zone value if you want to utilize time zone support.\\n\\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\\n\\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\\n\\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\\n\\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\\n\\tat com.mysql.cj.exceptions.ExceptionFactory.createException(ExceptionFactory.java:61)\\n\\tat com.mysql.cj.exceptions.ExceptionFactory.createException(ExceptionFactory.java:85)\\n\\tat com.mysql.cj.util.TimeUtil.getCanonicalTimezone(TimeUtil.java:132)\\n\\tat com.mysql.cj.protocol.a.NativeProtocol.configureTimezone(NativeProtocol.java:2120)\\n\\tat com.mysql.cj.protocol.a.NativeProtocol.initServerSession(NativeProtocol.java:2143)\\n\\tat com.mysql.cj.jdbc.ConnectionImpl.initializePropsFromServer(ConnectionImpl.java:1310)\\n\\tat com.mysql.cj.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:967)\\n\\tat com.mysql.cj.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:826)\\n\\t... 77 more\\n\\n'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371008d2-337f-4024-88b1-3d9cf3aabc5b",
   "metadata": {
    "tags": []
   },
   "source": [
    "'[jdbcSink] Iteration: 2, Status: Terminated with exception: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\\n  File \"/opt/conda/lib/python3.9/site-packages/py4j/clientserver.py\", line 581, in _call_proxy\\n    return_value = getattr(self.pool[obj_id], method)(*params)\\n  File \"/opt/conda/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 196, in call\\n    raise e\\n  File \"/opt/conda/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 193, in call\\n    self.func(DataFrame(jdf, self.sql_ctx), batch_id)\\n  File \"/tmp/ipykernel_3274/3087820090.py\", line 15, in saveToMySql\\n    writer.save()\\n  File \"/opt/conda/lib/python3.9/site-packages/pyspark/sql/readwriter.py\", line 738, in save\\n    self._jwrite.save()\\n  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\\n    return_value = get_return_value(\\n  File \"/opt/conda/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 111, in deco\\n    return f(*a, **kw)\\n  File \"/opt/conda/lib/python3.9/site-packages/py4j/protocol.py\", line 326, in get_return_value\\n    raise Py4JJavaError(\\npy4j.protocol.Py4JJavaError: An error occurred while calling o1162.save.\\n: java.sql.SQLSyntaxErrorException: Access denied for user \\'scott\\'@\\'%\\' to database \\'default&serverTimezone=Asia/Seoul\\'\\n\\tat com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120)\\n\\tat com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:97)\\n\\tat com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)\\n\\tat com.mysql.cj.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:836)\\n\\tat com.mysql.cj.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:456)\\n\\tat com.mysql.cj.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:246)\\n\\tat com.mysql.cj.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:197)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProvider$.create(ConnectionProvider.scala:77)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$createConnectionFactory$1(JdbcUtils.scala:64)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:49)\\n\\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\\n\\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\\n\\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\\n\\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\\n\\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\\n\\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)\\n\\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\\n\\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\\n\\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)\\n\\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)\\n\\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:128)\\n\\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\\n\\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\\n\\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\\n\\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\\n\\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\\n\\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\\n\\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\\n\\tat com.sun.proxy.$Proxy22.call(Unknown Source)\\n\\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:55)\\n\\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:55)\\n\\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:35)\\n\\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:600)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\\n\\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:598)\\n\\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\\n\\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\\n\\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\\n\\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:598)\\n\\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:228)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\\n\\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\\n\\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\\n\\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\\n\\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:193)\\n\\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)\\n\\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:187)\\n\\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:303)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\\n\\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:286)\\n\\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:209)\\n\\n'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f19256-3604-40ab-9665-2b3b99ca1d6a",
   "metadata": {
    "tags": []
   },
   "source": [
    "'[jdbcSink] Iteration: 2, Status: Terminated with exception: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\\n  File \"/opt/conda/lib/python3.9/site-packages/py4j/clientserver.py\", line 581, in _call_proxy\\n    return_value = getattr(self.pool[obj_id], method)(*params)\\n  File \"/opt/conda/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 196, in call\\n    raise e\\n  File \"/opt/conda/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 193, in call\\n    self.func(DataFrame(jdf, self.sql_ctx), batch_id)\\n  File \"/tmp/ipykernel_3274/3471113438.py\", line 13, in saveToMySql\\n    writer.save()\\n  File \"/opt/conda/lib/python3.9/site-packages/pyspark/sql/readwriter.py\", line 738, in save\\n    self._jwrite.save()\\n  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\\n    return_value = get_return_value(\\n  File \"/opt/conda/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 111, in deco\\n    return f(*a, **kw)\\n  File \"/opt/conda/lib/python3.9/site-packages/py4j/protocol.py\", line 326, in get_return_value\\n    raise Py4JJavaError(\\npy4j.protocol.Py4JJavaError: An error occurred while calling o1200.save.\\n: java.sql.SQLSyntaxErrorException: Access denied for user \\'scott\\'@\\'%\\' to database \\'&serverTimezdefaultone=Asia/Seoul\\'\\n\\tat com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120)\\n\\tat com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:97)\\n\\tat com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)\\n\\tat com.mysql.cj.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:836)\\n\\tat com.mysql.cj.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:456)\\n\\tat com.mysql.cj.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:246)\\n\\tat com.mysql.cj.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:197)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProvider$.create(ConnectionProvider.scala:77)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$createConnectionFactory$1(JdbcUtils.scala:64)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:49)\\n\\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\\n\\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\\n\\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\\n\\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\\n\\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\\n\\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)\\n\\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\\n\\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\\n\\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)\\n\\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)\\n\\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:128)\\n\\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\\n\\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\\n\\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\\n\\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\\n\\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\\n\\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\\n\\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\\n\\tat com.sun.proxy.$Proxy22.call(Unknown Source)\\n\\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:55)\\n\\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:55)\\n\\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:35)\\n\\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:600)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\\n\\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:598)\\n\\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\\n\\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\\n\\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\\n\\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:598)\\n\\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:228)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\\n\\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\\n\\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\\n\\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\\n\\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:193)\\n\\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)\\n\\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:187)\\n\\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:303)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\\n\\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:286)\\n\\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:209)\\n\\n'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6755f5-f342-4d50-9cf4-af3dd19d6ce3",
   "metadata": {
    "tags": []
   },
   "source": [
    "'[jdbcSink] Iteration: 6, Status: Terminated with exception: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\\n  File \"/opt/conda/lib/python3.9/site-packages/py4j/clientserver.py\", line 581, in _call_proxy\\n    return_value = getattr(self.pool[obj_id], method)(*params)\\n  File \"/opt/conda/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 196, in call\\n    raise e\\n  File \"/opt/conda/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 193, in call\\n    self.func(DataFrame(jdf, self.sql_ctx), batch_id)\\n  File \"/tmp/ipykernel_3844/8052112.py\", line 14, in saveToMySql\\n    writer.save()\\n  File \"/opt/conda/lib/python3.9/site-packages/pyspark/sql/readwriter.py\", line 738, in save\\n    self._jwrite.save()\\n  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\\n    return_value = get_return_value(\\n  File \"/opt/conda/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\\n    raise converted from None\\npyspark.sql.utils.IllegalArgumentException: Can\\'t get JDBC type for struct<start:timestamp,end:timestamp>\\n'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc6278c-fc74-4a85-ae1a-0344d4c3d05f",
   "metadata": {
    "tags": []
   },
   "source": [
    "'[jdbcSink] Iteration: 12, Status: Terminated with exception: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\\n  File \"/opt/conda/lib/python3.9/site-packages/py4j/clientserver.py\", line 581, in _call_proxy\\n    return_value = getattr(self.pool[obj_id], method)(*params)\\n  File \"/opt/conda/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 196, in call\\n    raise e\\n  File \"/opt/conda/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 193, in call\\n    self.func(DataFrame(jdf, self.sql_ctx), batch_id)\\n  File \"/tmp/ipykernel_3844/3582072675.py\", line 7, in saveToMySql\\n    dataFrame\\n  File \"/opt/conda/lib/python3.9/site-packages/pyspark/sql/readwriter.py\", line 568, in mode\\n    self._jwrite = self._jwrite.mode(saveMode)\\n  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\\n    return_value = get_return_value(\\n  File \"/opt/conda/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 117, in deco\\n    raise converted from None\\npyspark.sql.utils.IllegalArgumentException: Unknown save mode: update. Accepted save modes are \\'overwrite\\', \\'append\\', \\'ignore\\', \\'error\\', \\'errorifexists\\', \\'default\\'.\\n'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "52297a48-50fb-4850-882f-372242a6d325",
   "metadata": {},
   "outputs": [],
   "source": [
    "jdbcQuery.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
