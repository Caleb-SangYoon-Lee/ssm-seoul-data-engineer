{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d18f2b39-5f97-41d0-9c9a-31ec0b7a2bda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://2948b73de7f0:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fe520080e20>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from IPython.display import display, display_pretty, clear_output, JSON\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.sql.session.timeZone\", \"Asia/Seoul\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# 노트북에서 테이블 형태로 데이터 프레임 출력을 위한 설정을 합니다\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # display enabled\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.truncate\", 100) # display output columns size\n",
    "\n",
    "# 공통 데이터 위치\n",
    "home_jovyan = \"/home/jovyan\"\n",
    "work_data = f\"{home_jovyan}/work/data\"\n",
    "work_dir=!pwd\n",
    "work_dir = work_dir[0]\n",
    "\n",
    "# 로컬 환경 최적화\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5) # the number of partitions to use when shuffling data for joins or aggregations.\n",
    "spark.conf.set(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\")\n",
    "\n",
    "# 현재 기동된 스파크 애플리케이션의 포트를 확인하기 위해 스파크 정보를 출력합니다\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b0e4035-38f0-474e-8000-faf2d4c4802d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스트림 테이블을 주기적으로 조회하는 함수 (name: 이름, sql: Spark SQL, iterations: 반복횟수, sleep_secs: 인터벌)\n",
    "def displayStream(name, sql, iterations, sleep_secs):\n",
    "    from time import sleep\n",
    "    i = 1\n",
    "    for x in range(iterations):\n",
    "        clear_output(wait=True)              # 출력 Cell 을 지웁니다\n",
    "        display('[' + name + '] Iteration: '+str(i)+', Query: '+sql)\n",
    "        display(spark.sql(sql))              # Spark SQL 을 수행합니다\n",
    "        sleep(sleep_secs)                    # sleep_secs 초 만큼 대기합니다\n",
    "        i += 1\n",
    "\n",
    "# 스트림 쿼리의 상태를 주기적으로 조회하는 함수 (name: 이름, query: Streaming Query, iterations: 반복횟수, sleep_secs: 인터벌)\n",
    "def displayStatus(name, query, iterations, sleep_secs):\n",
    "    from time import sleep\n",
    "    i = 1\n",
    "    for x in range(iterations):\n",
    "        clear_output(wait=True)      # Output Cell 의 내용을 지웁니다\n",
    "        display('[' + name + '] Iteration: '+str(i)+', Status: '+query.status['message'])\n",
    "        display(query.lastProgress)  # 마지막 수행된 쿼리의 상태를 출력합니다\n",
    "        sleep(sleep_secs)            # 지정된 시간(초)을 대기합니다\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37d8b8bc-13c5-43a5-81f2-c8307e44582c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- mod_id: long (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- hello: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kafkaReader = (\n",
    "    spark\n",
    "  .readStream\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka:9093\")\n",
    "  .option(\"subscribe\", \"events\")\n",
    "  .option(\"startingOffsets\", \"latest\")\n",
    "  .load()\n",
    ")\n",
    "kafkaReader.printSchema()\n",
    "\n",
    "kafkaSchema = (\n",
    "    StructType()\n",
    "    .add(StructField(\"id\", LongType()))\n",
    "    .add(StructField(\"hello\", StringType()))\n",
    "    .add(StructField(\"time\", StringType()))\n",
    ")\n",
    "\n",
    "kafkaSelector = (\n",
    "    kafkaReader\n",
    "    .select(\n",
    "        col(\"key\").cast(\"string\"),\n",
    "        from_json(col(\"value\").cast(\"string\"), kafkaSchema).alias(\"events\")\n",
    "    )\n",
    "    .selectExpr(\"events.id % 10 as mod_id\", \"events.*\")\n",
    ")\n",
    "\n",
    "kafkaSelector.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "499824f1-177b-468d-9d78-91f176af8491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- uid: long (nullable = true)\n",
      "\n",
      "+-----+---+\n",
      "|name |uid|\n",
      "+-----+---+\n",
      "|zero |0  |\n",
      "|one  |1  |\n",
      "|two  |2  |\n",
      "|three|3  |\n",
      "|four |4  |\n",
      "|five |5  |\n",
      "|six  |6  |\n",
      "|seven|7  |\n",
      "|eight|8  |\n",
      "|nine |9  |\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "namePath = f\"{work_dir}/data/names\"\n",
    "nameStatic = (\n",
    "    spark\n",
    "    .read\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .json(namePath)\n",
    ")\n",
    "nameStatic.printSchema()\n",
    "nameStatic.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9d0f952-0609-4b11-bea6-84b6347e72ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "joinExpression = (kafkaSelector.mod_id == nameStatic.uid)\n",
    "kappaSelector = kafkaSelector.join(nameStatic, joinExpression, \"leftOuter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87ce2c8f-aae8-42ec-b09e-0f5e946d28a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "queryName = \"memorySink\"\n",
    "staticWriter = (\n",
    "    kappaSelector\n",
    "    .selectExpr(\"time\", \"id as user_id\", \"name as user_name\", \"hello\", \"uid\")\n",
    "    .selectExpr(\"user_id as key\", \"to_json(struct(*)) as value\")\n",
    "    .writeStream\n",
    "    .queryName(queryName)\n",
    "    .format(\"memory\")\n",
    "    .outputMode(\"append\")\n",
    ")\n",
    "\n",
    "checkpointLocation = f\"{work_dir}/tmp/{queryName}\"\n",
    "!rm -rf $checkpointLocation\n",
    "\n",
    "staticTrigger = (\n",
    "    staticWriter\n",
    "    .trigger(processingTime=\"5 second\")\n",
    "    .option(\"checkpointLocation\", checkpointLocation)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0f458e4-e856-42a5-b62f-3e28b737eb51",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[memorySink] Iteration: 40, Query: select * from memorySink order by key desc'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>key</th><th>value</th></tr>\n",
       "<tr><td>931</td><td>{&quot;time&quot;:&quot;2022-09-30 22:10:55&quot;,&quot;user_id&quot;:931,&quot;user_name&quot;:&quot;one&quot;,&quot;hello&quot;:&quot;ssm-seoul&quot;,&quot;uid&quot;:1}</td></tr>\n",
       "<tr><td>930</td><td>{&quot;time&quot;:&quot;2022-09-30 22:10:54&quot;,&quot;user_id&quot;:930,&quot;user_name&quot;:&quot;zero&quot;,&quot;hello&quot;:&quot;ssm-seoul&quot;,&quot;uid&quot;:0}</td></tr>\n",
       "<tr><td>929</td><td>{&quot;time&quot;:&quot;2022-09-30 22:10:53&quot;,&quot;user_id&quot;:929,&quot;user_name&quot;:&quot;nine&quot;,&quot;hello&quot;:&quot;ssm-seoul&quot;,&quot;uid&quot;:9}</td></tr>\n",
       "<tr><td>928</td><td>{&quot;time&quot;:&quot;2022-09-30 22:10:52&quot;,&quot;user_id&quot;:928,&quot;user_name&quot;:&quot;eight&quot;,&quot;hello&quot;:&quot;ssm-seoul&quot;,&quot;uid&quot;:8}</td></tr>\n",
       "<tr><td>927</td><td>{&quot;time&quot;:&quot;2022-09-30 22:10:51&quot;,&quot;user_id&quot;:927,&quot;user_name&quot;:&quot;seven&quot;,&quot;hello&quot;:&quot;ssm-seoul&quot;,&quot;uid&quot;:7}</td></tr>\n",
       "<tr><td>926</td><td>{&quot;time&quot;:&quot;2022-09-30 22:10:50&quot;,&quot;user_id&quot;:926,&quot;user_name&quot;:&quot;six&quot;,&quot;hello&quot;:&quot;ssm-seoul&quot;,&quot;uid&quot;:6}</td></tr>\n",
       "<tr><td>925</td><td>{&quot;time&quot;:&quot;2022-09-30 22:10:49&quot;,&quot;user_id&quot;:925,&quot;user_name&quot;:&quot;five&quot;,&quot;hello&quot;:&quot;ssm-seoul&quot;,&quot;uid&quot;:5}</td></tr>\n",
       "<tr><td>924</td><td>{&quot;time&quot;:&quot;2022-09-30 22:10:48&quot;,&quot;user_id&quot;:924,&quot;user_name&quot;:&quot;four&quot;,&quot;hello&quot;:&quot;ssm-seoul&quot;,&quot;uid&quot;:4}</td></tr>\n",
       "<tr><td>923</td><td>{&quot;time&quot;:&quot;2022-09-30 22:10:47&quot;,&quot;user_id&quot;:923,&quot;user_name&quot;:&quot;three&quot;,&quot;hello&quot;:&quot;ssm-seoul&quot;,&quot;uid&quot;:3}</td></tr>\n",
       "<tr><td>922</td><td>{&quot;time&quot;:&quot;2022-09-30 22:10:46&quot;,&quot;user_id&quot;:922,&quot;user_name&quot;:&quot;two&quot;,&quot;hello&quot;:&quot;ssm-seoul&quot;,&quot;uid&quot;:2}</td></tr>\n",
       "<tr><td>921</td><td>{&quot;time&quot;:&quot;2022-09-30 22:10:45&quot;,&quot;user_id&quot;:921,&quot;user_name&quot;:&quot;one&quot;,&quot;hello&quot;:&quot;ssm-seoul&quot;,&quot;uid&quot;:1}</td></tr>\n",
       "<tr><td>920</td><td>{&quot;time&quot;:&quot;2022-09-30 22:10:44&quot;,&quot;user_id&quot;:920,&quot;user_name&quot;:&quot;zero&quot;,&quot;hello&quot;:&quot;ssm-seoul&quot;,&quot;uid&quot;:0}</td></tr>\n",
       "<tr><td>919</td><td>{&quot;time&quot;:&quot;2022-09-30 22:10:43&quot;,&quot;user_id&quot;:919,&quot;user_name&quot;:&quot;nine&quot;,&quot;hello&quot;:&quot;ssm-seoul&quot;,&quot;uid&quot;:9}</td></tr>\n",
       "<tr><td>918</td><td>{&quot;time&quot;:&quot;2022-09-30 22:10:42&quot;,&quot;user_id&quot;:918,&quot;user_name&quot;:&quot;eight&quot;,&quot;hello&quot;:&quot;ssm-seoul&quot;,&quot;uid&quot;:8}</td></tr>\n",
       "<tr><td>917</td><td>{&quot;time&quot;:&quot;2022-09-30 22:10:41&quot;,&quot;user_id&quot;:917,&quot;user_name&quot;:&quot;seven&quot;,&quot;hello&quot;:&quot;ssm-seoul&quot;,&quot;uid&quot;:7}</td></tr>\n",
       "<tr><td>916</td><td>{&quot;time&quot;:&quot;2022-09-30 22:10:40&quot;,&quot;user_id&quot;:916,&quot;user_name&quot;:&quot;six&quot;,&quot;hello&quot;:&quot;ssm-seoul&quot;,&quot;uid&quot;:6}</td></tr>\n",
       "<tr><td>915</td><td>{&quot;time&quot;:&quot;2022-09-30 22:10:39&quot;,&quot;user_id&quot;:915,&quot;user_name&quot;:&quot;five&quot;,&quot;hello&quot;:&quot;ssm-seoul&quot;,&quot;uid&quot;:5}</td></tr>\n",
       "<tr><td>914</td><td>{&quot;time&quot;:&quot;2022-09-30 22:10:38&quot;,&quot;user_id&quot;:914,&quot;user_name&quot;:&quot;four&quot;,&quot;hello&quot;:&quot;ssm-seoul&quot;,&quot;uid&quot;:4}</td></tr>\n",
       "<tr><td>913</td><td>{&quot;time&quot;:&quot;2022-09-30 22:10:37&quot;,&quot;user_id&quot;:913,&quot;user_name&quot;:&quot;three&quot;,&quot;hello&quot;:&quot;ssm-seoul&quot;,&quot;uid&quot;:3}</td></tr>\n",
       "<tr><td>912</td><td>{&quot;time&quot;:&quot;2022-09-30 22:10:36&quot;,&quot;user_id&quot;:912,&quot;user_name&quot;:&quot;two&quot;,&quot;hello&quot;:&quot;ssm-seoul&quot;,&quot;uid&quot;:2}</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+---+--------------------------------------------------------------------------------------------+\n",
       "|key|                                                                                       value|\n",
       "+---+--------------------------------------------------------------------------------------------+\n",
       "|931|  {\"time\":\"2022-09-30 22:10:55\",\"user_id\":931,\"user_name\":\"one\",\"hello\":\"ssm-seoul\",\"uid\":1}|\n",
       "|930| {\"time\":\"2022-09-30 22:10:54\",\"user_id\":930,\"user_name\":\"zero\",\"hello\":\"ssm-seoul\",\"uid\":0}|\n",
       "|929| {\"time\":\"2022-09-30 22:10:53\",\"user_id\":929,\"user_name\":\"nine\",\"hello\":\"ssm-seoul\",\"uid\":9}|\n",
       "|928|{\"time\":\"2022-09-30 22:10:52\",\"user_id\":928,\"user_name\":\"eight\",\"hello\":\"ssm-seoul\",\"uid\":8}|\n",
       "|927|{\"time\":\"2022-09-30 22:10:51\",\"user_id\":927,\"user_name\":\"seven\",\"hello\":\"ssm-seoul\",\"uid\":7}|\n",
       "|926|  {\"time\":\"2022-09-30 22:10:50\",\"user_id\":926,\"user_name\":\"six\",\"hello\":\"ssm-seoul\",\"uid\":6}|\n",
       "|925| {\"time\":\"2022-09-30 22:10:49\",\"user_id\":925,\"user_name\":\"five\",\"hello\":\"ssm-seoul\",\"uid\":5}|\n",
       "|924| {\"time\":\"2022-09-30 22:10:48\",\"user_id\":924,\"user_name\":\"four\",\"hello\":\"ssm-seoul\",\"uid\":4}|\n",
       "|923|{\"time\":\"2022-09-30 22:10:47\",\"user_id\":923,\"user_name\":\"three\",\"hello\":\"ssm-seoul\",\"uid\":3}|\n",
       "|922|  {\"time\":\"2022-09-30 22:10:46\",\"user_id\":922,\"user_name\":\"two\",\"hello\":\"ssm-seoul\",\"uid\":2}|\n",
       "|921|  {\"time\":\"2022-09-30 22:10:45\",\"user_id\":921,\"user_name\":\"one\",\"hello\":\"ssm-seoul\",\"uid\":1}|\n",
       "|920| {\"time\":\"2022-09-30 22:10:44\",\"user_id\":920,\"user_name\":\"zero\",\"hello\":\"ssm-seoul\",\"uid\":0}|\n",
       "|919| {\"time\":\"2022-09-30 22:10:43\",\"user_id\":919,\"user_name\":\"nine\",\"hello\":\"ssm-seoul\",\"uid\":9}|\n",
       "|918|{\"time\":\"2022-09-30 22:10:42\",\"user_id\":918,\"user_name\":\"eight\",\"hello\":\"ssm-seoul\",\"uid\":8}|\n",
       "|917|{\"time\":\"2022-09-30 22:10:41\",\"user_id\":917,\"user_name\":\"seven\",\"hello\":\"ssm-seoul\",\"uid\":7}|\n",
       "|916|  {\"time\":\"2022-09-30 22:10:40\",\"user_id\":916,\"user_name\":\"six\",\"hello\":\"ssm-seoul\",\"uid\":6}|\n",
       "|915| {\"time\":\"2022-09-30 22:10:39\",\"user_id\":915,\"user_name\":\"five\",\"hello\":\"ssm-seoul\",\"uid\":5}|\n",
       "|914| {\"time\":\"2022-09-30 22:10:38\",\"user_id\":914,\"user_name\":\"four\",\"hello\":\"ssm-seoul\",\"uid\":4}|\n",
       "|913|{\"time\":\"2022-09-30 22:10:37\",\"user_id\":913,\"user_name\":\"three\",\"hello\":\"ssm-seoul\",\"uid\":3}|\n",
       "|912|  {\"time\":\"2022-09-30 22:10:36\",\"user_id\":912,\"user_name\":\"two\",\"hello\":\"ssm-seoul\",\"uid\":2}|\n",
       "+---+--------------------------------------------------------------------------------------------+\n",
       "only showing top 20 rows"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "WriteToDataSourceV2 org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@633d50cf\n",
      "+- Project [user_id#68L AS key#75L, to_json(struct(time, time#28, user_id, user_id#68L, user_name, user_name#69, hello, hello#27, uid, uid#42L), Some(Asia/Seoul)) AS value#76]\n",
      "   +- Project [time#28, id#26L AS user_id#68L, name#41 AS user_name#69, hello#27, uid#42L]\n",
      "      +- Join LeftOuter, (mod_id#25L = uid#42L)\n",
      "         :- Project [(events#21.id % cast(10 as bigint)) AS mod_id#25L, events#21.id AS id#26L, events#21.hello AS hello#27, events#21.time AS time#28]\n",
      "         :  +- Project [cast(key#7 as string) AS key#22, from_json(StructField(id,LongType,true), StructField(hello,StringType,true), StructField(time,StringType,true), cast(value#8 as string), Some(Asia/Seoul)) AS events#21]\n",
      "         :     +- StreamingDataSourceV2Relation [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan@105bd15f, KafkaV2[Subscribe[events]], {\"events\":{\"0\":932}}, {\"events\":{\"0\":940}}\n",
      "         +- Relation [name#41,uid#42L] json\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "WriteToDataSourceV2 org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@633d50cf\n",
      "+- Project [user_id#68L AS key#75L, to_json(struct(time, time#28, user_id, user_id#68L, user_name, user_name#69, hello, hello#27, uid, uid#42L), Some(Asia/Seoul)) AS value#76]\n",
      "   +- Project [time#28, id#26L AS user_id#68L, name#41 AS user_name#69, hello#27, uid#42L]\n",
      "      +- Join LeftOuter, (mod_id#25L = uid#42L)\n",
      "         :- Project [(events#21.id % cast(10 as bigint)) AS mod_id#25L, events#21.id AS id#26L, events#21.hello AS hello#27, events#21.time AS time#28]\n",
      "         :  +- Project [cast(key#7 as string) AS key#22, from_json(StructField(id,LongType,true), StructField(hello,StringType,true), StructField(time,StringType,true), cast(value#8 as string), Some(Asia/Seoul)) AS events#21]\n",
      "         :     +- StreamingDataSourceV2Relation [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan@105bd15f, KafkaV2[Subscribe[events]], {\"events\":{\"0\":932}}, {\"events\":{\"0\":940}}\n",
      "         +- Relation [name#41,uid#42L] json\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "WriteToDataSourceV2 org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@633d50cf\n",
      "+- Project [id#26L AS key#75L, to_json(struct(time, time#28, user_id, id#26L, user_name, name#41, hello, hello#27, uid, uid#42L), Some(Asia/Seoul)) AS value#76]\n",
      "   +- Join LeftOuter, (mod_id#25L = uid#42L)\n",
      "      :- Project [(from_json(StructField(id,LongType,true), cast(value#8 as string), Some(Asia/Seoul)).id % 10) AS mod_id#25L, from_json(StructField(id,LongType,true), cast(value#8 as string), Some(Asia/Seoul)).id AS id#26L, from_json(StructField(hello,StringType,true), cast(value#8 as string), Some(Asia/Seoul)).hello AS hello#27, from_json(StructField(time,StringType,true), cast(value#8 as string), Some(Asia/Seoul)).time AS time#28]\n",
      "      :  +- StreamingDataSourceV2Relation [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan@105bd15f, KafkaV2[Subscribe[events]], {\"events\":{\"0\":932}}, {\"events\":{\"0\":940}}\n",
      "      +- Filter isnotnull(uid#42L)\n",
      "         +- Relation [name#41,uid#42L] json\n",
      "\n",
      "== Physical Plan ==\n",
      "WriteToDataSourceV2 org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@633d50cf, org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy$$Lambda$3489/0x00000008413ab440@5b902e1a\n",
      "+- Project [id#26L AS key#75L, to_json(struct(time, time#28, user_id, id#26L, user_name, name#41, hello, hello#27, uid, uid#42L), Some(Asia/Seoul)) AS value#76]\n",
      "   +- *(2) BroadcastHashJoin [mod_id#25L], [uid#42L], LeftOuter, BuildRight, false\n",
      "      :- Project [(from_json(StructField(id,LongType,true), cast(value#8 as string), Some(Asia/Seoul)).id % 10) AS mod_id#25L, from_json(StructField(id,LongType,true), cast(value#8 as string), Some(Asia/Seoul)).id AS id#26L, from_json(StructField(hello,StringType,true), cast(value#8 as string), Some(Asia/Seoul)).hello AS hello#27, from_json(StructField(time,StringType,true), cast(value#8 as string), Some(Asia/Seoul)).time AS time#28]\n",
      "      :  +- MicroBatchScan[key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13] class org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan\n",
      "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[1, bigint, false]),false), [id=#9852]\n",
      "         +- *(1) Filter isnotnull(uid#42L)\n",
      "            +- FileScan json [name#41,uid#42L] Batched: false, DataFilters: [isnotnull(uid#42L)], Format: JSON, Location: InMemoryFileIndex(1 paths)[file:/home/jovyan/work/data/names], PartitionFilters: [], PushedFilters: [IsNotNull(uid)], ReadSchema: struct<name:string,uid:bigint>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "staticQuery = staticTrigger.start()\n",
    "displayStream(queryName, f\"select * from {queryName} order by key desc\", 40, 5)\n",
    "staticQuery.explain(True)\n",
    "staticQuery.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e63a5d50-1235-43b6-9107-99adf50e8c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf $checkpointLocation\n",
    "staticQuery.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff37a6e0-49f0-43f3-b965-d5f782e6443b",
   "metadata": {},
   "outputs": [],
   "source": [
    "queryName = \"jdbcSink\"\n",
    "jdbcWriter = (\n",
    "    kappaSelector\n",
    "    .selectExpr(\"time\", \"id as user_id\", \"name as user_name\", \"hello\", \"uid\")\n",
    "    .selectExpr(\"user_id as key\", \"to_json(struct(*)) as value\")\n",
    "    .writeStream\n",
    "    .queryName(queryName)\n",
    "    .format(\"jdbc\")\n",
    "    .option(\"url\", \"jdbc:mysql://scott:tiger@mysql:3306/default\")\n",
    "    .option(\"driver\", \"com.mysql.jdbc.Driver\")\n",
    "    .option(\"dbtable\", \"events\")\n",
    "    .outputMode(\"append\")\n",
    ")\n",
    "\n",
    "checkpointLocation = f\"{work_dir}/tmp/{queryName}\"\n",
    "!rm -rf $checkpointLocation\n",
    "\n",
    "jdbcTrigger = (\n",
    "    jdbcWriter\n",
    "    .trigger(processingTime=\"5 second\")\n",
    "    .option(\"checkpointLocation\", checkpointLocation)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "902f3083-821b-4f76-8131-f9981de6936c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1029.start.\n: java.lang.UnsupportedOperationException: Data source jdbc does not support streamed writing\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.streamedOperatorUnsupportedByDataSourceError(QueryExecutionErrors.scala:430)\n\tat org.apache.spark.sql.execution.datasources.DataSource.createSink(DataSource.scala:330)\n\tat org.apache.spark.sql.streaming.DataStreamWriter.createV1Sink(DataStreamWriter.scala:432)\n\tat org.apache.spark.sql.streaming.DataStreamWriter.startInternal(DataStreamWriter.scala:399)\n\tat org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:248)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m jdbcQuery \u001b[38;5;241m=\u001b[39m \u001b[43mjdbcTrigger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m displayStatus(queryName, jdbcQuery, \u001b[38;5;241m1000\u001b[39m, \u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m      3\u001b[0m jdbcQuery\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pyspark/sql/streaming.py:1202\u001b[0m, in \u001b[0;36mDataStreamWriter.start\u001b[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[1;32m   1200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueryName(queryName)\n\u001b[1;32m   1201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sq(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1203\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39mstart(path))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1029.start.\n: java.lang.UnsupportedOperationException: Data source jdbc does not support streamed writing\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.streamedOperatorUnsupportedByDataSourceError(QueryExecutionErrors.scala:430)\n\tat org.apache.spark.sql.execution.datasources.DataSource.createSink(DataSource.scala:330)\n\tat org.apache.spark.sql.streaming.DataStreamWriter.createV1Sink(DataStreamWriter.scala:432)\n\tat org.apache.spark.sql.streaming.DataStreamWriter.startInternal(DataStreamWriter.scala:399)\n\tat org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:248)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "jdbcQuery = jdbcTrigger.start()\n",
    "displayStatus(queryName, jdbcQuery, 1000, 10)\n",
    "jdbcQuery.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b13a213e-ca3d-4950-baaa-e6ec4124c624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveToMySql(dataFrame, batchId):\n",
    "    url = \"jdbc:mysql://scott:tiger@mysql:3306/default?useUnicode=true&serverTimezone=Asia/Seoul\"\n",
    "        # .withColumn(\"batchId\", batchId)\n",
    "    writer = (\n",
    "        dataFrame\n",
    "        .write\n",
    "        .format(\"jdbc\")\n",
    "        .option(\"url\", url)\n",
    "        .option(\"dbtable\", \"events\")\n",
    "        .mode(\"append\")\n",
    "    )\n",
    "    writer.save()\n",
    "\n",
    "queryName = \"jdbcSink\"\n",
    "jdbcWriter = (\n",
    "    kappaSelector\n",
    "    .selectExpr(\"time\", \"id as user_id\", \"name as user_name\", \"hello\", \"uid\")\n",
    "    .writeStream\n",
    "    .queryName(queryName)\n",
    "    .foreachBatch(saveToMySql)\n",
    ")\n",
    "\n",
    "checkpointLocation = f\"{work_dir}/tmp/{queryName}\"\n",
    "!rm -rf $checkpointLocation\n",
    "\n",
    "jdbcTrigger = (\n",
    "    jdbcWriter\n",
    "    .trigger(processingTime=\"5 second\")\n",
    "    .option(\"checkpointLocation\", checkpointLocation)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e32d1cc-7b97-435b-80a1-02b9d7c35101",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[jdbcSink] Iteration: 8, Status: Waiting for next trigger'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'd9956832-f234-49e5-a49a-08632617e2c8',\n",
       " 'runId': '01023e1c-3f61-42cd-ae24-36ed912d99e8',\n",
       " 'name': 'jdbcSink',\n",
       " 'timestamp': '2022-09-30T14:04:35.000Z',\n",
       " 'batchId': 4,\n",
       " 'numInputRows': 8,\n",
       " 'inputRowsPerSecond': 1.6003200640128026,\n",
       " 'processedRowsPerSecond': 8.221993833504625,\n",
       " 'durationMs': {'addBatch': 803,\n",
       "  'getBatch': 0,\n",
       "  'latestOffset': 3,\n",
       "  'queryPlanning': 38,\n",
       "  'triggerExecution': 973,\n",
       "  'walCommit': 58},\n",
       " 'stateOperators': [],\n",
       " 'sources': [{'description': 'KafkaV2[Subscribe[events]]',\n",
       "   'startOffset': {'events': {'0': 4142}},\n",
       "   'endOffset': {'events': {'0': 4150}},\n",
       "   'latestOffset': {'events': {'0': 4150}},\n",
       "   'numInputRows': 8,\n",
       "   'inputRowsPerSecond': 1.6003200640128026,\n",
       "   'processedRowsPerSecond': 8.221993833504625,\n",
       "   'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
       "    'maxOffsetsBehindLatest': '0',\n",
       "    'minOffsetsBehindLatest': '0'}}],\n",
       " 'sink': {'description': 'ForeachBatchSink', 'numOutputRows': -1}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m jdbcQuery \u001b[38;5;241m=\u001b[39m jdbcTrigger\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdisplayStatus\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueryName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjdbcQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m jdbcQuery\u001b[38;5;241m.\u001b[39mstop()\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mdisplayStatus\u001b[0;34m(name, query, iterations, sleep_secs)\u001b[0m\n\u001b[1;32m     18\u001b[0m display(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m] Iteration: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(i)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, Status: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mquery\u001b[38;5;241m.\u001b[39mstatus[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     19\u001b[0m display(query\u001b[38;5;241m.\u001b[39mlastProgress)  \u001b[38;5;66;03m# 마지막 수행된 쿼리의 상태를 출력합니다\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43msleep_secs\u001b[49m\u001b[43m)\u001b[49m            \u001b[38;5;66;03m# 지정된 시간(초)을 대기합니다\u001b[39;00m\n\u001b[1;32m     21\u001b[0m i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "jdbcQuery = jdbcTrigger.start()\n",
    "displayStatus(queryName, jdbcQuery, 100, 3)\n",
    "jdbcQuery.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "52297a48-50fb-4850-882f-372242a6d325",
   "metadata": {},
   "outputs": [],
   "source": [
    "jdbcQuery.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e93533f-d8b5-457b-95c4-de568f6c4051",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "'[jdbcSink] Iteration: 33, Status: Terminated with exception: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\\n  File \"/opt/conda/lib/python3.9/site-packages/py4j/clientserver.py\", line 581, in _call_proxy\\n    return_value = getattr(self.pool[obj_id], method)(*params)\\n  File \"/opt/conda/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 196, in call\\n    raise e\\n  File \"/opt/conda/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 193, in call\\n    self.func(DataFrame(jdf, self.sql_ctx), batch_id)\\n  File \"/tmp/ipykernel_3274/1984705217.py\", line 14, in saveToMySql\\n    writer.save()\\n  File \"/opt/conda/lib/python3.9/site-packages/pyspark/sql/readwriter.py\", line 738, in save\\n    self._jwrite.save()\\n  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\\n    return_value = get_return_value(\\n  File \"/opt/conda/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 111, in deco\\n    return f(*a, **kw)\\n  File \"/opt/conda/lib/python3.9/site-packages/py4j/protocol.py\", line 326, in get_return_value\\n    raise Py4JJavaError(\\npy4j.protocol.Py4JJavaError: An error occurred while calling o1101.save.\\n: java.sql.SQLException: The server time zone value \\'KST\\' is unrecognized or represents more than one time zone. You must configure either the server or JDBC driver (via the \\'serverTimezone\\' configuration property) to use a more specifc time zone value if you want to utilize time zone support.\\n\\tat com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:129)\\n\\tat com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:97)\\n\\tat com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:89)\\n\\tat com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:63)\\n\\tat com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:73)\\n\\tat com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:76)\\n\\tat com.mysql.cj.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:836)\\n\\tat com.mysql.cj.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:456)\\n\\tat com.mysql.cj.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:246)\\n\\tat com.mysql.cj.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:197)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProvider$.create(ConnectionProvider.scala:77)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$createConnectionFactory$1(JdbcUtils.scala:64)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:49)\\n\\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\\n\\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\\n\\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\\n\\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\\n\\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\\n\\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)\\n\\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\\n\\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\\n\\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)\\n\\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)\\n\\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:128)\\n\\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\\n\\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\\n\\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\\n\\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\\n\\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\\n\\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\\n\\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\\n\\tat com.sun.proxy.$Proxy22.call(Unknown Source)\\n\\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:55)\\n\\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:55)\\n\\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:35)\\n\\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:600)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\\n\\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:598)\\n\\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\\n\\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\\n\\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\\n\\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:598)\\n\\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:228)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\\n\\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\\n\\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\\n\\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\\n\\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:193)\\n\\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)\\n\\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:187)\\n\\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:303)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\\n\\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:286)\\n\\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:209)\\nCaused by: com.mysql.cj.exceptions.InvalidConnectionAttributeException: The server time zone value \\'KST\\' is unrecognized or represents more than one time zone. You must configure either the server or JDBC driver (via the \\'serverTimezone\\' configuration property) to use a more specifc time zone value if you want to utilize time zone support.\\n\\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\\n\\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\\n\\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\\n\\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\\n\\tat com.mysql.cj.exceptions.ExceptionFactory.createException(ExceptionFactory.java:61)\\n\\tat com.mysql.cj.exceptions.ExceptionFactory.createException(ExceptionFactory.java:85)\\n\\tat com.mysql.cj.util.TimeUtil.getCanonicalTimezone(TimeUtil.java:132)\\n\\tat com.mysql.cj.protocol.a.NativeProtocol.configureTimezone(NativeProtocol.java:2120)\\n\\tat com.mysql.cj.protocol.a.NativeProtocol.initServerSession(NativeProtocol.java:2143)\\n\\tat com.mysql.cj.jdbc.ConnectionImpl.initializePropsFromServer(ConnectionImpl.java:1310)\\n\\tat com.mysql.cj.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:967)\\n\\tat com.mysql.cj.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:826)\\n\\t... 77 more\\n\\n'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371008d2-337f-4024-88b1-3d9cf3aabc5b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "'[jdbcSink] Iteration: 2, Status: Terminated with exception: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\\n  File \"/opt/conda/lib/python3.9/site-packages/py4j/clientserver.py\", line 581, in _call_proxy\\n    return_value = getattr(self.pool[obj_id], method)(*params)\\n  File \"/opt/conda/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 196, in call\\n    raise e\\n  File \"/opt/conda/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 193, in call\\n    self.func(DataFrame(jdf, self.sql_ctx), batch_id)\\n  File \"/tmp/ipykernel_3274/3087820090.py\", line 15, in saveToMySql\\n    writer.save()\\n  File \"/opt/conda/lib/python3.9/site-packages/pyspark/sql/readwriter.py\", line 738, in save\\n    self._jwrite.save()\\n  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\\n    return_value = get_return_value(\\n  File \"/opt/conda/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 111, in deco\\n    return f(*a, **kw)\\n  File \"/opt/conda/lib/python3.9/site-packages/py4j/protocol.py\", line 326, in get_return_value\\n    raise Py4JJavaError(\\npy4j.protocol.Py4JJavaError: An error occurred while calling o1162.save.\\n: java.sql.SQLSyntaxErrorException: Access denied for user \\'scott\\'@\\'%\\' to database \\'default&serverTimezone=Asia/Seoul\\'\\n\\tat com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120)\\n\\tat com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:97)\\n\\tat com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)\\n\\tat com.mysql.cj.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:836)\\n\\tat com.mysql.cj.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:456)\\n\\tat com.mysql.cj.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:246)\\n\\tat com.mysql.cj.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:197)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProvider$.create(ConnectionProvider.scala:77)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$createConnectionFactory$1(JdbcUtils.scala:64)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:49)\\n\\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\\n\\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\\n\\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\\n\\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\\n\\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\\n\\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)\\n\\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\\n\\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\\n\\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)\\n\\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)\\n\\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:128)\\n\\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\\n\\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\\n\\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\\n\\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\\n\\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\\n\\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\\n\\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\\n\\tat com.sun.proxy.$Proxy22.call(Unknown Source)\\n\\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:55)\\n\\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:55)\\n\\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:35)\\n\\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:600)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\\n\\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:598)\\n\\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\\n\\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\\n\\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\\n\\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:598)\\n\\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:228)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\\n\\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\\n\\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\\n\\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\\n\\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:193)\\n\\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)\\n\\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:187)\\n\\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:303)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\\n\\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:286)\\n\\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:209)\\n\\n'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f19256-3604-40ab-9665-2b3b99ca1d6a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "'[jdbcSink] Iteration: 2, Status: Terminated with exception: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\\n  File \"/opt/conda/lib/python3.9/site-packages/py4j/clientserver.py\", line 581, in _call_proxy\\n    return_value = getattr(self.pool[obj_id], method)(*params)\\n  File \"/opt/conda/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 196, in call\\n    raise e\\n  File \"/opt/conda/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 193, in call\\n    self.func(DataFrame(jdf, self.sql_ctx), batch_id)\\n  File \"/tmp/ipykernel_3274/3471113438.py\", line 13, in saveToMySql\\n    writer.save()\\n  File \"/opt/conda/lib/python3.9/site-packages/pyspark/sql/readwriter.py\", line 738, in save\\n    self._jwrite.save()\\n  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\\n    return_value = get_return_value(\\n  File \"/opt/conda/lib/python3.9/site-packages/pyspark/sql/utils.py\", line 111, in deco\\n    return f(*a, **kw)\\n  File \"/opt/conda/lib/python3.9/site-packages/py4j/protocol.py\", line 326, in get_return_value\\n    raise Py4JJavaError(\\npy4j.protocol.Py4JJavaError: An error occurred while calling o1200.save.\\n: java.sql.SQLSyntaxErrorException: Access denied for user \\'scott\\'@\\'%\\' to database \\'&serverTimezdefaultone=Asia/Seoul\\'\\n\\tat com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120)\\n\\tat com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:97)\\n\\tat com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)\\n\\tat com.mysql.cj.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:836)\\n\\tat com.mysql.cj.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:456)\\n\\tat com.mysql.cj.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:246)\\n\\tat com.mysql.cj.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:197)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProvider$.create(ConnectionProvider.scala:77)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$createConnectionFactory$1(JdbcUtils.scala:64)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:49)\\n\\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\\n\\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\\n\\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\\n\\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\\n\\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\\n\\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)\\n\\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\\n\\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\\n\\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)\\n\\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)\\n\\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:128)\\n\\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\\n\\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\\n\\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\\n\\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\\n\\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\\n\\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\\n\\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\\n\\tat com.sun.proxy.$Proxy22.call(Unknown Source)\\n\\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:55)\\n\\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:55)\\n\\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:35)\\n\\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:600)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\\n\\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:598)\\n\\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\\n\\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\\n\\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\\n\\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:598)\\n\\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:228)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\\n\\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\\n\\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\\n\\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\\n\\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:193)\\n\\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)\\n\\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:187)\\n\\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:303)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\\n\\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:286)\\n\\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:209)\\n\\n'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
